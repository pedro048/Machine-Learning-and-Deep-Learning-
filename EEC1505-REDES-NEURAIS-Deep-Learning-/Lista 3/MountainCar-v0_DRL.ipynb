{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc7d5be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.3.0 in /home/pedro/.local/lib/python3.8/site-packages (2.3.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow==2.3.0) (2.3.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.41.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow==2.3.0) (2.10.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.1.2)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow==2.3.0) (2.6.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.4.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow==2.3.0) (3.18.1)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.18.5)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow==2.3.0) (0.34.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.13.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.2.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.3.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow==2.3.0) (1.14.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow==2.3.0) (3.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow==2.3.0) (1.6.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow==2.3.0) (0.14.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/pedro/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/pedro/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.22.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/pedro/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.35.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (45.2.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/pedro/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/pedro/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/pedro/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/lib/python3/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/lib/python3/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/pedro/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.1.0)\n",
      "Requirement already satisfied: gym in /home/pedro/.local/lib/python3.8/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/pedro/.local/lib/python3.8/site-packages (from gym) (1.18.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/pedro/.local/lib/python3.8/site-packages (from gym) (2.0.0)\n",
      "Requirement already satisfied: keras in /home/pedro/.local/lib/python3.8/site-packages (2.6.0)\n",
      "Requirement already satisfied: keras-rl2 in /home/pedro/.local/lib/python3.8/site-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in /home/pedro/.local/lib/python3.8/site-packages (from keras-rl2) (2.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow->keras-rl2) (0.2.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.1.2)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow->keras-rl2) (2.6.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow->keras-rl2) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow->keras-rl2) (1.14.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow->keras-rl2) (2.3.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow->keras-rl2) (0.34.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.41.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow->keras-rl2) (0.3.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow->keras-rl2) (0.14.1)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.18.5)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.13.2)\n",
      "Requirement already satisfied: scipy==1.4.1 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow->keras-rl2) (1.4.1)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow->keras-rl2) (2.10.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/pedro/.local/lib/python3.8/site-packages (from tensorflow->keras-rl2) (3.18.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/pedro/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/pedro/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/pedro/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (2.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/pedro/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (45.2.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/pedro/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.8.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/pedro/.local/lib/python3.8/site-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.35.0)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (2.22.0)\r\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/pedro/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (1.3.0)\r\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/lib/python3/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (4.0.0)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (0.2.1)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/lib/python3/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (4.0)\r\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow->keras-rl2) (3.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.3.0\n",
    "!pip install gym\n",
    "!pip install keras\n",
    "!pip install keras-rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1bf0a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyglet in /home/pedro/.local/lib/python3.8/site-packages (1.5.21)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cedfb1b",
   "metadata": {},
   "source": [
    "# **Environment (MountainCar-v0) with OpenAI Gym**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afb6a1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d10bdea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a67bd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: 2\n"
     ]
    }
   ],
   "source": [
    "# Position\n",
    "# Velocity\n",
    "print('States:', states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "158a901d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions: 3\n"
     ]
    }
   ],
   "source": [
    "# Push left\n",
    "# Push right\n",
    "# No push\n",
    "print('actions:', actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942349ef",
   "metadata": {},
   "source": [
    "# **Test Before Use Deep Reinforcement Learning (DRL)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89724e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-200.0\n",
      "Episode:2 Score:-200.0\n",
      "Episode:3 Score:-200.0\n",
      "Episode:4 Score:-200.0\n",
      "Episode:5 Score:-200.0\n",
      "Episode:6 Score:-200.0\n",
      "Episode:7 Score:-200.0\n",
      "Episode:8 Score:-200.0\n",
      "Episode:9 Score:-200.0\n",
      "Episode:10 Score:-200.0\n",
      "Episode:11 Score:-200.0\n",
      "Episode:12 Score:-200.0\n",
      "Episode:13 Score:-200.0\n",
      "Episode:14 Score:-200.0\n",
      "Episode:15 Score:-200.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 15 # match number\n",
    "for episode in range(1, episodes+1):\n",
    "  state = env.reset()\n",
    "  done = False\n",
    "  score = 0\n",
    "\n",
    "  while not done:\n",
    "    env.render()\n",
    "    action = random.choice([0,1])\n",
    "    n_state, reward, done, info = env.step(action)\n",
    "    score+=reward\n",
    "  print('Episode:{} Score:{}'.format(episode, score)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d3c351",
   "metadata": {},
   "source": [
    "# **Deep Learning Model (MLP) with Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b82d3ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e03adf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP => 3 hidden layers (1ยบ = 128 neurons, 2ยบ = 64 neurons, 3ยบ = 32 neurons)\n",
    "#        2 neurons for each state in the input layer\n",
    "#        3 neurons for each action in the output layer\n",
    "#        activation function for all the hidden layers = relu \n",
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(env.action_space.n, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ae213e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               384       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 10,819\n",
      "Trainable params: 10,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# MLP structure\n",
    "model = build_model(states, actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b5c9e",
   "metadata": {},
   "source": [
    "# **Build Agent with Keras-DRL (Deep Reinforcement Learning)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d54ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97a97f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=env.action_space.n, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514edb47",
   "metadata": {},
   "source": [
    "# **DRL Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5afec40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 150000 steps ...\n",
      "WARNING:tensorflow:From /home/pedro/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.8/site-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    200/150000: episode: 1, duration: 3.389s, episode steps: 200, steps per second:  59, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.045511, mae: 0.912031, mean_q: -1.233350\n",
      "    400/150000: episode: 2, duration: 1.304s, episode steps: 200, steps per second: 153, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 0.004268, mae: 1.894394, mean_q: -2.791724\n",
      "    600/150000: episode: 3, duration: 1.840s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 0.029440, mae: 2.989484, mean_q: -4.391519\n",
      "    800/150000: episode: 4, duration: 2.390s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 0.043859, mae: 4.117525, mean_q: -6.063605\n",
      "   1000/150000: episode: 5, duration: 2.367s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 0.090332, mae: 5.248688, mean_q: -7.741448\n",
      "   1200/150000: episode: 6, duration: 2.981s, episode steps: 200, steps per second:  67, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 0.135578, mae: 6.350101, mean_q: -9.339651\n",
      "   1400/150000: episode: 7, duration: 2.117s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 0.188839, mae: 7.412508, mean_q: -10.946520\n",
      "   1600/150000: episode: 8, duration: 1.668s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 0.345517, mae: 8.427827, mean_q: -12.380451\n",
      "   1800/150000: episode: 9, duration: 1.549s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 0.312505, mae: 9.429473, mean_q: -13.920466\n",
      "   2000/150000: episode: 10, duration: 2.058s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 0.509786, mae: 10.382707, mean_q: -15.285823\n",
      "   2200/150000: episode: 11, duration: 1.735s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 0.552704, mae: 11.307566, mean_q: -16.649328\n",
      "   2400/150000: episode: 12, duration: 2.635s, episode steps: 200, steps per second:  76, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 0.752261, mae: 12.126362, mean_q: -17.844868\n",
      "   2600/150000: episode: 13, duration: 2.943s, episode steps: 200, steps per second:  68, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.870852, mae: 12.998318, mean_q: -19.164209\n",
      "   2800/150000: episode: 14, duration: 2.870s, episode steps: 200, steps per second:  70, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 1.165283, mae: 13.771880, mean_q: -20.265369\n",
      "   3000/150000: episode: 15, duration: 2.114s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 1.260102, mae: 14.514783, mean_q: -21.322329\n",
      "   3200/150000: episode: 16, duration: 2.055s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 1.251958, mae: 15.243482, mean_q: -22.458006\n",
      "   3400/150000: episode: 17, duration: 1.930s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 0.929147, mae: 15.974936, mean_q: -23.648975\n",
      "   3600/150000: episode: 18, duration: 1.618s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 1.550737, mae: 16.633541, mean_q: -24.523045\n",
      "   3800/150000: episode: 19, duration: 1.660s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 1.736586, mae: 17.241192, mean_q: -25.341026\n",
      "   4000/150000: episode: 20, duration: 2.094s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 1.808969, mae: 17.861731, mean_q: -26.334816\n",
      "   4200/150000: episode: 21, duration: 2.656s, episode steps: 200, steps per second:  75, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 1.445436, mae: 18.431042, mean_q: -27.214916\n",
      "   4400/150000: episode: 22, duration: 2.040s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 1.954270, mae: 18.960552, mean_q: -27.971933\n",
      "   4600/150000: episode: 23, duration: 2.284s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000],  loss: 1.588592, mae: 19.526592, mean_q: -28.813408\n",
      "   4800/150000: episode: 24, duration: 2.480s, episode steps: 200, steps per second:  81, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 1.283942, mae: 20.120211, mean_q: -29.705811\n",
      "   5000/150000: episode: 25, duration: 1.580s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 1.716055, mae: 20.574936, mean_q: -30.358160\n",
      "   5200/150000: episode: 26, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 1.985749, mae: 21.071688, mean_q: -31.076206\n",
      "   5400/150000: episode: 27, duration: 1.363s, episode steps: 200, steps per second: 147, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 2.495164, mae: 21.528984, mean_q: -31.706894\n",
      "   5600/150000: episode: 28, duration: 1.841s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 2.391334, mae: 21.909859, mean_q: -32.271397\n",
      "   5800/150000: episode: 29, duration: 1.864s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 2.691739, mae: 22.308718, mean_q: -32.914463\n",
      "   6000/150000: episode: 30, duration: 1.462s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 2.570809, mae: 22.632299, mean_q: -33.322269\n",
      "   6200/150000: episode: 31, duration: 2.321s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 2.893176, mae: 22.934454, mean_q: -33.797569\n",
      "   6400/150000: episode: 32, duration: 2.642s, episode steps: 200, steps per second:  76, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 2.681130, mae: 23.285084, mean_q: -34.405182\n",
      "   6600/150000: episode: 33, duration: 2.783s, episode steps: 200, steps per second:  72, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 3.053256, mae: 23.628252, mean_q: -34.840954\n",
      "   6800/150000: episode: 34, duration: 2.257s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 1.893146, mae: 24.014692, mean_q: -35.475903\n",
      "   7000/150000: episode: 35, duration: 1.352s, episode steps: 200, steps per second: 148, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.310 [0.000, 2.000],  loss: 3.217006, mae: 24.392267, mean_q: -35.860626\n",
      "   7200/150000: episode: 36, duration: 1.494s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.260 [0.000, 2.000],  loss: 3.518932, mae: 24.540066, mean_q: -36.121552\n",
      "   7400/150000: episode: 37, duration: 2.048s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000],  loss: 3.225387, mae: 24.841137, mean_q: -36.618359\n",
      "   7600/150000: episode: 38, duration: 1.574s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 3.620387, mae: 25.050903, mean_q: -36.922337\n",
      "   7800/150000: episode: 39, duration: 1.335s, episode steps: 200, steps per second: 150, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 1.976055, mae: 25.406069, mean_q: -37.603271\n",
      "   8000/150000: episode: 40, duration: 1.476s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 3.556165, mae: 25.692226, mean_q: -37.885220\n",
      "   8200/150000: episode: 41, duration: 1.888s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 2.734900, mae: 25.985323, mean_q: -38.398495\n",
      "   8400/150000: episode: 42, duration: 1.710s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.200 [0.000, 2.000],  loss: 3.114450, mae: 26.307722, mean_q: -38.850445\n",
      "   8600/150000: episode: 43, duration: 1.625s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 3.176425, mae: 26.637938, mean_q: -39.325417\n",
      "   8800/150000: episode: 44, duration: 1.829s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 3.457448, mae: 26.882097, mean_q: -39.629017\n",
      "   9000/150000: episode: 45, duration: 1.482s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 3.555691, mae: 27.173969, mean_q: -40.070156\n",
      "   9200/150000: episode: 46, duration: 1.578s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 3.201949, mae: 27.340454, mean_q: -40.405060\n",
      "   9400/150000: episode: 47, duration: 1.391s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 4.728663, mae: 27.613609, mean_q: -40.720551\n",
      "   9600/150000: episode: 48, duration: 1.568s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 3.790537, mae: 27.774086, mean_q: -41.018875\n",
      "   9800/150000: episode: 49, duration: 1.382s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 3.600374, mae: 28.019556, mean_q: -41.351131\n",
      "  10000/150000: episode: 50, duration: 1.706s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 3.722936, mae: 28.173567, mean_q: -41.540604\n",
      "  10200/150000: episode: 51, duration: 1.909s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 3.696449, mae: 28.322168, mean_q: -41.936172\n",
      "  10400/150000: episode: 52, duration: 2.042s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.180 [0.000, 2.000],  loss: 3.860584, mae: 28.640930, mean_q: -42.240700\n",
      "  10600/150000: episode: 53, duration: 3.205s, episode steps: 200, steps per second:  62, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 4.403121, mae: 28.894873, mean_q: -42.730698\n",
      "  10800/150000: episode: 54, duration: 2.253s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 5.059208, mae: 29.168789, mean_q: -43.004528\n",
      "  11000/150000: episode: 55, duration: 2.885s, episode steps: 200, steps per second:  69, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.220 [0.000, 2.000],  loss: 5.459268, mae: 29.295969, mean_q: -43.251289\n",
      "  11200/150000: episode: 56, duration: 1.903s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 3.239816, mae: 29.465357, mean_q: -43.578495\n",
      "  11400/150000: episode: 57, duration: 1.444s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000],  loss: 3.809189, mae: 29.717762, mean_q: -43.878178\n",
      "  11600/150000: episode: 58, duration: 1.476s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 6.544084, mae: 29.866377, mean_q: -43.970741\n",
      "  11800/150000: episode: 59, duration: 1.622s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 4.926462, mae: 30.107447, mean_q: -44.486725\n",
      "  12000/150000: episode: 60, duration: 1.941s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 3.782827, mae: 30.318699, mean_q: -44.895546\n",
      "  12200/150000: episode: 61, duration: 1.679s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 6.437773, mae: 30.580200, mean_q: -45.173515\n",
      "  12400/150000: episode: 62, duration: 1.597s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 4.441282, mae: 30.833330, mean_q: -45.610859\n",
      "  12600/150000: episode: 63, duration: 1.505s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 5.695277, mae: 31.089483, mean_q: -45.981968\n",
      "  12800/150000: episode: 64, duration: 1.751s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 5.498419, mae: 31.255465, mean_q: -46.259090\n",
      "  13000/150000: episode: 65, duration: 1.895s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 5.701993, mae: 31.461216, mean_q: -46.543007\n",
      "  13200/150000: episode: 66, duration: 1.876s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 7.070180, mae: 31.708458, mean_q: -46.867603\n",
      "  13400/150000: episode: 67, duration: 1.505s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 7.456467, mae: 31.749340, mean_q: -46.819870\n",
      "  13600/150000: episode: 68, duration: 1.397s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 5.343390, mae: 31.918329, mean_q: -47.257492\n",
      "  13800/150000: episode: 69, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 5.000173, mae: 32.191772, mean_q: -47.647461\n",
      "  14000/150000: episode: 70, duration: 1.488s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 5.038466, mae: 32.473660, mean_q: -48.101124\n",
      "  14200/150000: episode: 71, duration: 2.620s, episode steps: 200, steps per second:  76, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 4.725093, mae: 32.725609, mean_q: -48.510967\n",
      "  14400/150000: episode: 72, duration: 2.044s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 5.932834, mae: 32.958778, mean_q: -48.783951\n",
      "  14600/150000: episode: 73, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 7.088531, mae: 33.140858, mean_q: -48.950233\n",
      "  14800/150000: episode: 74, duration: 2.540s, episode steps: 200, steps per second:  79, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 5.558363, mae: 33.324799, mean_q: -49.429424\n",
      "  15000/150000: episode: 75, duration: 2.338s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 6.751520, mae: 33.550766, mean_q: -49.596058\n",
      "  15200/150000: episode: 76, duration: 1.637s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 5.440516, mae: 33.765068, mean_q: -50.015072\n",
      "  15400/150000: episode: 77, duration: 1.664s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 7.062243, mae: 34.069992, mean_q: -50.399464\n",
      "  15600/150000: episode: 78, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 6.239601, mae: 34.273457, mean_q: -50.633854\n",
      "  15800/150000: episode: 79, duration: 1.856s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 6.912569, mae: 34.396172, mean_q: -50.909615\n",
      "  16000/150000: episode: 80, duration: 1.949s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 5.980948, mae: 34.698231, mean_q: -51.422863\n",
      "  16200/150000: episode: 81, duration: 1.516s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 6.799548, mae: 34.826550, mean_q: -51.606220\n",
      "  16400/150000: episode: 82, duration: 1.788s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 7.052145, mae: 35.063976, mean_q: -51.824547\n",
      "  16600/150000: episode: 83, duration: 1.569s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 5.936543, mae: 35.257652, mean_q: -52.231201\n",
      "  16800/150000: episode: 84, duration: 1.573s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 6.510334, mae: 35.461685, mean_q: -52.560802\n",
      "  17000/150000: episode: 85, duration: 1.368s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 8.152013, mae: 35.462753, mean_q: -52.457829\n",
      "  17200/150000: episode: 86, duration: 1.631s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 5.598901, mae: 35.680069, mean_q: -52.943272\n",
      "  17400/150000: episode: 87, duration: 1.556s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 5.503690, mae: 35.949413, mean_q: -53.232906\n",
      "  17600/150000: episode: 88, duration: 1.565s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 6.850021, mae: 36.126102, mean_q: -53.617287\n",
      "  17800/150000: episode: 89, duration: 1.525s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 7.059622, mae: 36.270859, mean_q: -53.732353\n",
      "  18000/150000: episode: 90, duration: 1.608s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 8.224171, mae: 36.405251, mean_q: -53.898106\n",
      "  18200/150000: episode: 91, duration: 1.385s, episode steps: 200, steps per second: 144, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 5.916484, mae: 36.626816, mean_q: -54.349258\n",
      "  18400/150000: episode: 92, duration: 1.563s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 6.796970, mae: 36.774765, mean_q: -54.498798\n",
      "  18600/150000: episode: 93, duration: 1.380s, episode steps: 200, steps per second: 145, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 7.784090, mae: 36.985966, mean_q: -54.750645\n",
      "  18800/150000: episode: 94, duration: 1.404s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 7.375449, mae: 37.059391, mean_q: -54.878685\n",
      "  19000/150000: episode: 95, duration: 1.413s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 8.673046, mae: 37.178165, mean_q: -54.948399\n",
      "  19200/150000: episode: 96, duration: 2.643s, episode steps: 200, steps per second:  76, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 7.428177, mae: 37.299770, mean_q: -55.279911\n",
      "  19400/150000: episode: 97, duration: 2.310s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 7.279213, mae: 37.441505, mean_q: -55.429386\n",
      "  19600/150000: episode: 98, duration: 1.882s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 7.511694, mae: 37.524311, mean_q: -55.588036\n",
      "  19800/150000: episode: 99, duration: 1.959s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 9.094606, mae: 37.663330, mean_q: -55.807430\n",
      "  20000/150000: episode: 100, duration: 1.984s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 9.005478, mae: 37.547146, mean_q: -55.618797\n",
      "  20200/150000: episode: 101, duration: 1.872s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 9.474981, mae: 37.542377, mean_q: -55.581085\n",
      "  20400/150000: episode: 102, duration: 1.952s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 6.804772, mae: 37.721359, mean_q: -55.885166\n",
      "  20600/150000: episode: 103, duration: 1.783s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 9.126082, mae: 37.819859, mean_q: -55.890614\n",
      "  20800/150000: episode: 104, duration: 2.155s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 6.174158, mae: 37.816628, mean_q: -56.122700\n",
      "  21000/150000: episode: 105, duration: 2.204s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 7.375300, mae: 37.872856, mean_q: -56.112137\n",
      "  21200/150000: episode: 106, duration: 1.767s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 7.789018, mae: 38.058994, mean_q: -56.500187\n",
      "  21400/150000: episode: 107, duration: 2.095s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 7.265490, mae: 38.277103, mean_q: -56.846214\n",
      "  21600/150000: episode: 108, duration: 1.988s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 6.583298, mae: 38.390347, mean_q: -56.880493\n",
      "  21800/150000: episode: 109, duration: 1.498s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 9.586715, mae: 38.392975, mean_q: -56.718174\n",
      "  22000/150000: episode: 110, duration: 1.695s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 9.460157, mae: 38.410973, mean_q: -56.818836\n",
      "  22200/150000: episode: 111, duration: 1.934s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 9.092336, mae: 38.390560, mean_q: -56.766571\n",
      "  22400/150000: episode: 112, duration: 1.832s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 7.811077, mae: 38.416279, mean_q: -56.976112\n",
      "  22600/150000: episode: 113, duration: 1.650s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 7.345825, mae: 38.572136, mean_q: -57.239883\n",
      "  22800/150000: episode: 114, duration: 1.484s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 8.949089, mae: 38.628765, mean_q: -57.181324\n",
      "  23000/150000: episode: 115, duration: 1.455s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 7.658096, mae: 38.691368, mean_q: -57.389091\n",
      "  23200/150000: episode: 116, duration: 1.374s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.923204, mae: 38.845333, mean_q: -57.512939\n",
      "  23400/150000: episode: 117, duration: 1.558s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 9.045621, mae: 38.713085, mean_q: -57.352032\n",
      "  23600/150000: episode: 118, duration: 1.432s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 5.941792, mae: 38.934574, mean_q: -57.831154\n",
      "  23800/150000: episode: 119, duration: 1.550s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 7.919061, mae: 39.057056, mean_q: -57.889523\n",
      "  24000/150000: episode: 120, duration: 1.374s, episode steps: 200, steps per second: 146, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 8.155337, mae: 39.211796, mean_q: -57.994553\n",
      "  24200/150000: episode: 121, duration: 1.622s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 8.623205, mae: 39.253384, mean_q: -58.120918\n",
      "  24400/150000: episode: 122, duration: 2.486s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 6.885514, mae: 39.216965, mean_q: -58.162941\n",
      "  24600/150000: episode: 123, duration: 2.015s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 8.442723, mae: 39.388813, mean_q: -58.354580\n",
      "  24800/150000: episode: 124, duration: 1.516s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 7.653460, mae: 39.377289, mean_q: -58.372215\n",
      "  25000/150000: episode: 125, duration: 1.465s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 8.527626, mae: 39.539722, mean_q: -58.549061\n",
      "  25200/150000: episode: 126, duration: 2.066s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 7.054625, mae: 39.678516, mean_q: -58.883759\n",
      "  25400/150000: episode: 127, duration: 1.394s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 11.070165, mae: 39.768444, mean_q: -58.805283\n",
      "  25600/150000: episode: 128, duration: 1.961s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 6.706222, mae: 39.778404, mean_q: -58.993423\n",
      "  25800/150000: episode: 129, duration: 1.627s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 4.983887, mae: 39.916191, mean_q: -59.372128\n",
      "  26000/150000: episode: 130, duration: 1.895s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 13.233169, mae: 39.837658, mean_q: -58.686699\n",
      "  26200/150000: episode: 131, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 9.536482, mae: 39.807419, mean_q: -58.855553\n",
      "  26400/150000: episode: 132, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 6.665801, mae: 39.928864, mean_q: -59.296913\n",
      "  26600/150000: episode: 133, duration: 1.808s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 9.514415, mae: 39.979740, mean_q: -59.204056\n",
      "  26800/150000: episode: 134, duration: 1.442s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 9.409377, mae: 39.971531, mean_q: -59.142677\n",
      "  27000/150000: episode: 135, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 10.190825, mae: 40.045422, mean_q: -59.132896\n",
      "  27200/150000: episode: 136, duration: 1.693s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 8.044400, mae: 40.089043, mean_q: -59.466141\n",
      "  27400/150000: episode: 137, duration: 2.518s, episode steps: 200, steps per second:  79, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 9.485304, mae: 40.076324, mean_q: -59.269608\n",
      "  27600/150000: episode: 138, duration: 2.860s, episode steps: 200, steps per second:  70, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 9.529593, mae: 40.001907, mean_q: -59.220879\n",
      "  27800/150000: episode: 139, duration: 2.524s, episode steps: 200, steps per second:  79, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 10.527205, mae: 39.978325, mean_q: -59.184959\n",
      "  28000/150000: episode: 140, duration: 1.987s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 7.673344, mae: 40.076191, mean_q: -59.417423\n",
      "  28200/150000: episode: 141, duration: 1.884s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 5.593492, mae: 40.225468, mean_q: -59.774628\n",
      "  28400/150000: episode: 142, duration: 1.874s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 6.683273, mae: 40.324253, mean_q: -59.933418\n",
      "  28600/150000: episode: 143, duration: 1.544s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 9.048169, mae: 40.357841, mean_q: -59.655453\n",
      "  28800/150000: episode: 144, duration: 1.696s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 7.636862, mae: 40.421757, mean_q: -59.894863\n",
      "  29000/150000: episode: 145, duration: 2.297s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 10.972696, mae: 40.351627, mean_q: -59.631397\n",
      "  29200/150000: episode: 146, duration: 1.462s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 10.104963, mae: 40.300533, mean_q: -59.715099\n",
      "  29400/150000: episode: 147, duration: 1.416s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 9.225071, mae: 40.341801, mean_q: -59.710606\n",
      "  29600/150000: episode: 148, duration: 1.693s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 10.993039, mae: 40.262413, mean_q: -59.529747\n",
      "  29800/150000: episode: 149, duration: 1.867s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 7.581401, mae: 40.287651, mean_q: -59.804214\n",
      "  30000/150000: episode: 150, duration: 1.552s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 9.875454, mae: 40.340080, mean_q: -59.822742\n",
      "  30200/150000: episode: 151, duration: 1.522s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 9.479688, mae: 40.268246, mean_q: -59.504429\n",
      "  30400/150000: episode: 152, duration: 1.623s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 8.232830, mae: 40.193390, mean_q: -59.602402\n",
      "  30600/150000: episode: 153, duration: 1.602s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 8.292995, mae: 40.208794, mean_q: -59.678402\n",
      "  30800/150000: episode: 154, duration: 1.500s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 6.934677, mae: 40.303883, mean_q: -59.826126\n",
      "  31000/150000: episode: 155, duration: 1.554s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 7.682809, mae: 40.360287, mean_q: -59.973911\n",
      "  31200/150000: episode: 156, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 9.651397, mae: 40.449268, mean_q: -59.878819\n",
      "  31400/150000: episode: 157, duration: 1.614s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 10.348447, mae: 40.495461, mean_q: -59.949669\n",
      "  31600/150000: episode: 158, duration: 1.483s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 7.861504, mae: 40.445518, mean_q: -60.009541\n",
      "  31800/150000: episode: 159, duration: 1.582s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 10.533600, mae: 40.495827, mean_q: -59.889278\n",
      "  32000/150000: episode: 160, duration: 1.881s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 8.465369, mae: 40.527332, mean_q: -59.986481\n",
      "  32200/150000: episode: 161, duration: 1.894s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 9.663872, mae: 40.473595, mean_q: -59.989246\n",
      "  32400/150000: episode: 162, duration: 1.511s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.810 [0.000, 2.000],  loss: 7.288037, mae: 40.498161, mean_q: -60.037033\n",
      "  32600/150000: episode: 163, duration: 2.138s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 6.806581, mae: 40.582664, mean_q: -60.320000\n",
      "  32800/150000: episode: 164, duration: 1.795s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 10.586074, mae: 40.685616, mean_q: -60.103973\n",
      "  33000/150000: episode: 165, duration: 1.616s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 9.150739, mae: 40.665173, mean_q: -60.278008\n",
      "  33200/150000: episode: 166, duration: 1.600s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 6.323099, mae: 40.807327, mean_q: -60.638828\n",
      "  33400/150000: episode: 167, duration: 1.580s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 8.437421, mae: 40.975483, mean_q: -60.837658\n",
      "  33600/150000: episode: 168, duration: 1.518s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 11.125872, mae: 41.045616, mean_q: -60.670021\n",
      "  33800/150000: episode: 169, duration: 1.511s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 10.937614, mae: 40.889565, mean_q: -60.473545\n",
      "  34000/150000: episode: 170, duration: 1.434s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 7.727430, mae: 40.903072, mean_q: -60.786621\n",
      "  34200/150000: episode: 171, duration: 1.700s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 10.930593, mae: 40.854923, mean_q: -60.435631\n",
      "  34400/150000: episode: 172, duration: 1.529s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 6.872077, mae: 40.780392, mean_q: -60.512089\n",
      "  34600/150000: episode: 173, duration: 2.131s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.035915, mae: 40.947487, mean_q: -60.791481\n",
      "  34800/150000: episode: 174, duration: 2.453s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 10.747576, mae: 40.895050, mean_q: -60.447285\n",
      "  35000/150000: episode: 175, duration: 1.801s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 8.522772, mae: 40.884674, mean_q: -60.570457\n",
      "  35200/150000: episode: 176, duration: 1.593s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 7.649517, mae: 40.925041, mean_q: -60.709980\n",
      "  35400/150000: episode: 177, duration: 2.047s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 8.844381, mae: 40.961758, mean_q: -60.549717\n",
      "  35600/150000: episode: 178, duration: 1.757s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 7.412039, mae: 40.806034, mean_q: -60.461159\n",
      "  35800/150000: episode: 179, duration: 1.622s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 10.047899, mae: 40.799980, mean_q: -60.444359\n",
      "  36000/150000: episode: 180, duration: 1.791s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 8.118039, mae: 40.912659, mean_q: -60.709671\n",
      "  36200/150000: episode: 181, duration: 1.704s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.815 [0.000, 2.000],  loss: 8.089547, mae: 40.908321, mean_q: -60.632946\n",
      "  36400/150000: episode: 182, duration: 1.692s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 6.407860, mae: 40.985634, mean_q: -60.786633\n",
      "  36600/150000: episode: 183, duration: 1.746s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 10.131657, mae: 40.967972, mean_q: -60.632130\n",
      "  36800/150000: episode: 184, duration: 1.901s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 10.597979, mae: 41.009415, mean_q: -60.630558\n",
      "  37000/150000: episode: 185, duration: 1.440s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 7.260219, mae: 40.984303, mean_q: -60.730473\n",
      "  37200/150000: episode: 186, duration: 1.593s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 10.259491, mae: 41.081963, mean_q: -60.777168\n",
      "  37400/150000: episode: 187, duration: 1.541s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 7.703746, mae: 41.019608, mean_q: -60.939510\n",
      "  37600/150000: episode: 188, duration: 1.567s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 10.759891, mae: 41.059254, mean_q: -60.610615\n",
      "  37800/150000: episode: 189, duration: 1.667s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 12.084011, mae: 41.032581, mean_q: -60.651134\n",
      "  38000/150000: episode: 190, duration: 1.457s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 10.746106, mae: 40.902977, mean_q: -60.527958\n",
      "  38200/150000: episode: 191, duration: 1.400s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 9.845814, mae: 40.692955, mean_q: -60.264870\n",
      "  38400/150000: episode: 192, duration: 1.765s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 9.987613, mae: 40.693058, mean_q: -60.226543\n",
      "  38600/150000: episode: 193, duration: 1.434s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.415997, mae: 40.669464, mean_q: -60.211002\n",
      "  38800/150000: episode: 194, duration: 1.627s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 7.936005, mae: 40.718567, mean_q: -60.349220\n",
      "  39000/150000: episode: 195, duration: 1.639s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 8.260799, mae: 40.735439, mean_q: -60.429100\n",
      "  39200/150000: episode: 196, duration: 1.396s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 8.612749, mae: 40.761757, mean_q: -60.392681\n",
      "  39400/150000: episode: 197, duration: 1.407s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 10.074734, mae: 40.773647, mean_q: -60.361996\n",
      "  39600/150000: episode: 198, duration: 1.470s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 9.926774, mae: 40.680088, mean_q: -60.199268\n",
      "  39800/150000: episode: 199, duration: 1.803s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 7.929645, mae: 40.647873, mean_q: -60.246769\n",
      "  40000/150000: episode: 200, duration: 1.526s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 9.958454, mae: 40.683441, mean_q: -60.243996\n",
      "  40200/150000: episode: 201, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 8.382528, mae: 40.668617, mean_q: -60.254025\n",
      "  40400/150000: episode: 202, duration: 1.483s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 8.998503, mae: 40.688526, mean_q: -60.272530\n",
      "  40600/150000: episode: 203, duration: 2.858s, episode steps: 200, steps per second:  70, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 8.501300, mae: 40.737598, mean_q: -60.395626\n",
      "  40800/150000: episode: 204, duration: 3.454s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 10.688009, mae: 40.720860, mean_q: -60.328968\n",
      "  41000/150000: episode: 205, duration: 3.124s, episode steps: 200, steps per second:  64, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 8.809300, mae: 40.766758, mean_q: -60.490158\n",
      "  41200/150000: episode: 206, duration: 3.309s, episode steps: 200, steps per second:  60, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 8.262149, mae: 40.843117, mean_q: -60.571609\n",
      "  41400/150000: episode: 207, duration: 2.165s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 11.413711, mae: 40.814404, mean_q: -60.332447\n",
      "  41600/150000: episode: 208, duration: 2.068s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 10.026556, mae: 40.812286, mean_q: -60.358509\n",
      "  41800/150000: episode: 209, duration: 2.634s, episode steps: 200, steps per second:  76, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 9.199074, mae: 40.749687, mean_q: -60.416073\n",
      "  42000/150000: episode: 210, duration: 3.193s, episode steps: 200, steps per second:  63, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 8.686048, mae: 40.723560, mean_q: -60.403976\n",
      "  42200/150000: episode: 211, duration: 2.949s, episode steps: 200, steps per second:  68, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 6.876511, mae: 40.749493, mean_q: -60.484844\n",
      "  42400/150000: episode: 212, duration: 2.695s, episode steps: 200, steps per second:  74, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 9.149603, mae: 40.845245, mean_q: -60.404285\n",
      "  42600/150000: episode: 213, duration: 1.966s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 10.782336, mae: 40.755123, mean_q: -60.378765\n",
      "  42800/150000: episode: 214, duration: 2.030s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 8.304300, mae: 40.740772, mean_q: -60.447216\n",
      "  43000/150000: episode: 215, duration: 2.213s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 7.336482, mae: 40.796303, mean_q: -60.559628\n",
      "  43200/150000: episode: 216, duration: 3.237s, episode steps: 200, steps per second:  62, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 9.439289, mae: 40.832218, mean_q: -60.499271\n",
      "  43400/150000: episode: 217, duration: 3.621s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 8.991543, mae: 40.916206, mean_q: -60.674984\n",
      "  43600/150000: episode: 218, duration: 2.625s, episode steps: 200, steps per second:  76, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 7.509744, mae: 41.003628, mean_q: -60.849190\n",
      "  43800/150000: episode: 219, duration: 1.789s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 10.713103, mae: 41.085430, mean_q: -60.860909\n",
      "  44000/150000: episode: 220, duration: 1.788s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 8.880507, mae: 41.126675, mean_q: -60.953697\n",
      "  44200/150000: episode: 221, duration: 1.773s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 10.129688, mae: 41.085518, mean_q: -60.839325\n",
      "  44400/150000: episode: 222, duration: 2.021s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 6.723889, mae: 41.101845, mean_q: -60.972237\n",
      "  44600/150000: episode: 223, duration: 1.767s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 8.045712, mae: 41.175137, mean_q: -61.145226\n",
      "  44800/150000: episode: 224, duration: 1.712s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 10.548626, mae: 41.249802, mean_q: -61.010677\n",
      "  45000/150000: episode: 225, duration: 2.915s, episode steps: 200, steps per second:  69, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 8.523301, mae: 41.135532, mean_q: -60.983383\n",
      "  45200/150000: episode: 226, duration: 3.460s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 7.614913, mae: 41.239708, mean_q: -61.301369\n",
      "  45400/150000: episode: 227, duration: 2.747s, episode steps: 200, steps per second:  73, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 8.128775, mae: 41.370899, mean_q: -61.302280\n",
      "  45600/150000: episode: 228, duration: 1.772s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 9.857099, mae: 41.446056, mean_q: -61.500229\n",
      "  45800/150000: episode: 229, duration: 2.577s, episode steps: 200, steps per second:  78, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 8.694808, mae: 41.339249, mean_q: -61.293690\n",
      "  46000/150000: episode: 230, duration: 3.489s, episode steps: 200, steps per second:  57, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 11.848797, mae: 41.296097, mean_q: -61.082920\n",
      "  46200/150000: episode: 231, duration: 3.652s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 8.178476, mae: 41.257362, mean_q: -61.163631\n",
      "  46400/150000: episode: 232, duration: 2.006s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 11.738489, mae: 41.237961, mean_q: -61.003651\n",
      "  46600/150000: episode: 233, duration: 1.744s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 9.073687, mae: 41.142235, mean_q: -60.981323\n",
      "  46800/150000: episode: 234, duration: 1.833s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 13.058225, mae: 41.059052, mean_q: -60.694740\n",
      "  47000/150000: episode: 235, duration: 1.935s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 9.572136, mae: 40.894745, mean_q: -60.502480\n",
      "  47200/150000: episode: 236, duration: 1.729s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 9.698345, mae: 40.839317, mean_q: -60.466915\n",
      "  47400/150000: episode: 237, duration: 1.771s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 9.164692, mae: 40.896103, mean_q: -60.619091\n",
      "  47600/150000: episode: 238, duration: 2.063s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 12.165125, mae: 40.772751, mean_q: -60.366879\n",
      "  47800/150000: episode: 239, duration: 1.865s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 12.663653, mae: 40.626095, mean_q: -60.067501\n",
      "  48000/150000: episode: 240, duration: 1.847s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 8.797892, mae: 40.517902, mean_q: -60.093311\n",
      "  48200/150000: episode: 241, duration: 1.726s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 11.195597, mae: 40.513309, mean_q: -59.890160\n",
      "  48400/150000: episode: 242, duration: 1.734s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 11.574854, mae: 40.377697, mean_q: -59.739269\n",
      "  48600/150000: episode: 243, duration: 1.969s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 9.667136, mae: 40.316891, mean_q: -59.670555\n",
      "  48800/150000: episode: 244, duration: 1.775s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 8.670280, mae: 40.256821, mean_q: -59.723774\n",
      "  49000/150000: episode: 245, duration: 1.726s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 8.200907, mae: 40.360474, mean_q: -59.886456\n",
      "  49200/150000: episode: 246, duration: 2.209s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 9.133471, mae: 40.353584, mean_q: -59.775742\n",
      "  49400/150000: episode: 247, duration: 1.924s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 8.341457, mae: 40.439495, mean_q: -59.893375\n",
      "  49600/150000: episode: 248, duration: 2.164s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 8.993979, mae: 40.439713, mean_q: -59.945576\n",
      "  49800/150000: episode: 249, duration: 1.635s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 7.513321, mae: 40.471729, mean_q: -60.089371\n",
      "  50000/150000: episode: 250, duration: 1.460s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 6.537372, mae: 40.611935, mean_q: -60.363327\n",
      "  50200/150000: episode: 251, duration: 1.470s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 11.376121, mae: 40.566116, mean_q: -59.912399\n",
      "  50400/150000: episode: 252, duration: 1.466s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 8.566606, mae: 40.464657, mean_q: -59.971024\n",
      "  50600/150000: episode: 253, duration: 1.454s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 11.347570, mae: 40.344757, mean_q: -59.661129\n",
      "  50800/150000: episode: 254, duration: 1.633s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 7.090828, mae: 40.260117, mean_q: -59.679276\n",
      "  51000/150000: episode: 255, duration: 1.702s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 12.051737, mae: 40.137291, mean_q: -59.436806\n",
      "  51200/150000: episode: 256, duration: 1.635s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 8.454362, mae: 40.246086, mean_q: -59.710590\n",
      "  51400/150000: episode: 257, duration: 2.134s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 8.233909, mae: 40.273163, mean_q: -59.653439\n",
      "  51600/150000: episode: 258, duration: 3.423s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 4.452432, mae: 40.407578, mean_q: -60.088589\n",
      "  51800/150000: episode: 259, duration: 2.173s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 5.000331, mae: 40.462906, mean_q: -60.177193\n",
      "  52000/150000: episode: 260, duration: 2.799s, episode steps: 200, steps per second:  71, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 10.422733, mae: 40.504025, mean_q: -59.970505\n",
      "  52200/150000: episode: 261, duration: 2.181s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 10.203686, mae: 40.453991, mean_q: -59.862011\n",
      "  52400/150000: episode: 262, duration: 2.610s, episode steps: 200, steps per second:  77, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 8.092031, mae: 40.358738, mean_q: -59.809414\n",
      "  52600/150000: episode: 263, duration: 3.473s, episode steps: 200, steps per second:  58, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 12.030107, mae: 40.306164, mean_q: -59.553101\n",
      "  52800/150000: episode: 264, duration: 3.665s, episode steps: 200, steps per second:  55, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 9.092230, mae: 40.303818, mean_q: -59.738976\n",
      "  53000/150000: episode: 265, duration: 3.325s, episode steps: 200, steps per second:  60, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 7.277966, mae: 40.373661, mean_q: -59.922867\n",
      "  53200/150000: episode: 266, duration: 1.912s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 8.366036, mae: 40.527451, mean_q: -60.085659\n",
      "  53400/150000: episode: 267, duration: 1.995s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 8.016414, mae: 40.539757, mean_q: -60.078007\n",
      "  53600/150000: episode: 268, duration: 2.496s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 10.054560, mae: 40.520634, mean_q: -59.983917\n",
      "  53800/150000: episode: 269, duration: 3.414s, episode steps: 200, steps per second:  59, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 10.445898, mae: 40.496361, mean_q: -59.903896\n",
      "  54000/150000: episode: 270, duration: 2.736s, episode steps: 200, steps per second:  73, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 9.253451, mae: 40.538162, mean_q: -60.109219\n",
      "  54200/150000: episode: 271, duration: 2.098s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 8.168317, mae: 40.549030, mean_q: -60.127193\n",
      "  54400/150000: episode: 272, duration: 2.197s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 7.556204, mae: 40.449577, mean_q: -60.057674\n",
      "  54600/150000: episode: 273, duration: 2.238s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 12.037385, mae: 40.350044, mean_q: -59.551270\n",
      "  54800/150000: episode: 274, duration: 1.802s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 8.565537, mae: 40.153404, mean_q: -59.494083\n",
      "  55000/150000: episode: 275, duration: 1.491s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 8.633204, mae: 40.269299, mean_q: -59.774853\n",
      "  55200/150000: episode: 276, duration: 1.466s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 9.234331, mae: 40.303177, mean_q: -59.630863\n",
      "  55400/150000: episode: 277, duration: 1.475s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 8.694845, mae: 40.354855, mean_q: -59.836349\n",
      "  55600/150000: episode: 278, duration: 1.683s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 10.100805, mae: 40.382801, mean_q: -59.778877\n",
      "  55800/150000: episode: 279, duration: 1.560s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 12.135777, mae: 40.214745, mean_q: -59.493286\n",
      "  56000/150000: episode: 280, duration: 2.051s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 8.675222, mae: 40.244431, mean_q: -59.738453\n",
      "  56200/150000: episode: 281, duration: 2.245s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 7.136547, mae: 40.293251, mean_q: -59.782791\n",
      "  56400/150000: episode: 282, duration: 2.456s, episode steps: 200, steps per second:  81, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 9.528374, mae: 40.386425, mean_q: -59.901855\n",
      "  56600/150000: episode: 283, duration: 2.080s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 6.227392, mae: 40.308067, mean_q: -59.822052\n",
      "  56800/150000: episode: 284, duration: 1.805s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 7.336826, mae: 40.452091, mean_q: -60.020687\n",
      "  57000/150000: episode: 285, duration: 2.204s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 9.603700, mae: 40.521641, mean_q: -60.038467\n",
      "  57200/150000: episode: 286, duration: 2.122s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 10.406994, mae: 40.466297, mean_q: -59.909988\n",
      "  57400/150000: episode: 287, duration: 2.469s, episode steps: 200, steps per second:  81, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 10.278081, mae: 40.352783, mean_q: -59.666855\n",
      "  57600/150000: episode: 288, duration: 2.323s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 12.430449, mae: 40.283089, mean_q: -59.572720\n",
      "  57800/150000: episode: 289, duration: 2.314s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 9.444544, mae: 40.257027, mean_q: -59.691288\n",
      "  58000/150000: episode: 290, duration: 1.989s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 7.303892, mae: 40.330494, mean_q: -59.795822\n",
      "  58200/150000: episode: 291, duration: 2.252s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 9.324924, mae: 40.373390, mean_q: -59.771935\n",
      "  58400/150000: episode: 292, duration: 1.927s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.318023, mae: 40.379829, mean_q: -59.918320\n",
      "  58600/150000: episode: 293, duration: 1.721s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 7.479829, mae: 40.348034, mean_q: -59.832886\n",
      "  58800/150000: episode: 294, duration: 1.688s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 7.146577, mae: 40.513977, mean_q: -60.110947\n",
      "  59000/150000: episode: 295, duration: 1.752s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 6.875279, mae: 40.549549, mean_q: -60.149918\n",
      "  59200/150000: episode: 296, duration: 2.000s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 9.765402, mae: 40.547089, mean_q: -59.952148\n",
      "  59400/150000: episode: 297, duration: 1.956s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 10.062884, mae: 40.460491, mean_q: -59.818119\n",
      "  59600/150000: episode: 298, duration: 1.742s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 8.406883, mae: 40.558254, mean_q: -60.156063\n",
      "  59800/150000: episode: 299, duration: 1.608s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 7.631669, mae: 40.675968, mean_q: -60.402176\n",
      "  60000/150000: episode: 300, duration: 1.879s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 12.463675, mae: 40.654972, mean_q: -60.075615\n",
      "  60200/150000: episode: 301, duration: 1.651s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 6.404240, mae: 40.580772, mean_q: -60.173752\n",
      "  60400/150000: episode: 302, duration: 1.678s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 9.294539, mae: 40.636341, mean_q: -60.309853\n",
      "  60600/150000: episode: 303, duration: 2.032s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 10.015256, mae: 40.597878, mean_q: -60.086620\n",
      "  60800/150000: episode: 304, duration: 1.701s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 9.896412, mae: 40.613541, mean_q: -60.096237\n",
      "  61000/150000: episode: 305, duration: 2.325s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 9.815959, mae: 40.670719, mean_q: -60.278778\n",
      "  61200/150000: episode: 306, duration: 1.877s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 5.834764, mae: 40.723339, mean_q: -60.606464\n",
      "  61400/150000: episode: 307, duration: 1.603s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 9.341266, mae: 40.830563, mean_q: -60.391899\n",
      "  61600/150000: episode: 308, duration: 1.821s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 10.273689, mae: 40.692104, mean_q: -60.205292\n",
      "  61800/150000: episode: 309, duration: 2.318s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 10.090708, mae: 40.509731, mean_q: -59.933372\n",
      "  62000/150000: episode: 310, duration: 2.672s, episode steps: 200, steps per second:  75, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 7.777141, mae: 40.487507, mean_q: -60.006229\n",
      "  62200/150000: episode: 311, duration: 1.649s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 11.639153, mae: 40.462246, mean_q: -59.820625\n",
      "  62400/150000: episode: 312, duration: 1.876s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.570885, mae: 40.432491, mean_q: -59.990074\n",
      "  62600/150000: episode: 313, duration: 1.573s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 8.888202, mae: 40.520107, mean_q: -60.115780\n",
      "  62800/150000: episode: 314, duration: 2.005s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 10.362227, mae: 40.431084, mean_q: -59.867054\n",
      "  63000/150000: episode: 315, duration: 1.832s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 8.895409, mae: 40.386185, mean_q: -59.768711\n",
      "  63200/150000: episode: 316, duration: 1.834s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 10.213671, mae: 40.365425, mean_q: -59.711647\n",
      "  63400/150000: episode: 317, duration: 1.473s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.695 [0.000, 2.000],  loss: 10.675172, mae: 40.239803, mean_q: -59.361778\n",
      "  63600/150000: episode: 318, duration: 2.384s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 8.114159, mae: 40.031578, mean_q: -59.270615\n",
      "  63800/150000: episode: 319, duration: 1.623s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 10.180734, mae: 40.081467, mean_q: -59.299145\n",
      "  64000/150000: episode: 320, duration: 1.572s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 10.018450, mae: 40.027897, mean_q: -59.209976\n",
      "  64200/150000: episode: 321, duration: 1.738s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 10.996468, mae: 39.876507, mean_q: -59.029221\n",
      "  64400/150000: episode: 322, duration: 1.500s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 8.532703, mae: 39.877151, mean_q: -59.186230\n",
      "  64600/150000: episode: 323, duration: 2.292s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 7.323524, mae: 39.898632, mean_q: -59.201981\n",
      "  64800/150000: episode: 324, duration: 2.579s, episode steps: 200, steps per second:  78, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 9.585989, mae: 39.758595, mean_q: -58.933929\n",
      "  65000/150000: episode: 325, duration: 2.646s, episode steps: 200, steps per second:  76, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 8.732506, mae: 39.770184, mean_q: -58.922081\n",
      "  65200/150000: episode: 326, duration: 1.767s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 11.271858, mae: 39.667816, mean_q: -58.633678\n",
      "  65400/150000: episode: 327, duration: 1.835s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 8.663241, mae: 39.659737, mean_q: -58.705391\n",
      "  65600/150000: episode: 328, duration: 2.022s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 8.150100, mae: 39.691010, mean_q: -58.927422\n",
      "  65800/150000: episode: 329, duration: 2.050s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 6.798602, mae: 39.625210, mean_q: -58.796398\n",
      "  66000/150000: episode: 330, duration: 1.834s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 7.948106, mae: 39.620163, mean_q: -58.807514\n",
      "  66200/150000: episode: 331, duration: 2.274s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 6.958103, mae: 39.708664, mean_q: -58.916523\n",
      "  66400/150000: episode: 332, duration: 2.287s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 9.988608, mae: 39.627697, mean_q: -58.622921\n",
      "  66600/150000: episode: 333, duration: 2.121s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 9.937675, mae: 39.498104, mean_q: -58.496864\n",
      "  66800/150000: episode: 334, duration: 2.069s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 7.712551, mae: 39.468933, mean_q: -58.560928\n",
      "  67000/150000: episode: 335, duration: 2.074s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 9.221319, mae: 39.453873, mean_q: -58.349232\n",
      "  67200/150000: episode: 336, duration: 2.013s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 9.425292, mae: 39.356224, mean_q: -58.296406\n",
      "  67400/150000: episode: 337, duration: 2.015s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 7.239626, mae: 39.361591, mean_q: -58.356544\n",
      "  67600/150000: episode: 338, duration: 1.890s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 10.950191, mae: 39.312767, mean_q: -58.088943\n",
      "  67800/150000: episode: 339, duration: 2.280s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 10.963136, mae: 39.228146, mean_q: -58.139404\n",
      "  68000/150000: episode: 340, duration: 1.806s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 8.035242, mae: 39.239395, mean_q: -58.191940\n",
      "  68200/150000: episode: 341, duration: 2.205s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 5.125886, mae: 39.268681, mean_q: -58.283546\n",
      "  68400/150000: episode: 342, duration: 1.880s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 7.915753, mae: 39.333717, mean_q: -58.343765\n",
      "  68600/150000: episode: 343, duration: 2.129s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 7.122602, mae: 39.401283, mean_q: -58.430771\n",
      "  68800/150000: episode: 344, duration: 1.540s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 7.491794, mae: 39.527538, mean_q: -58.541466\n",
      "  69000/150000: episode: 345, duration: 1.877s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 6.109999, mae: 39.650078, mean_q: -58.868935\n",
      "  69200/150000: episode: 346, duration: 1.857s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.776228, mae: 39.669674, mean_q: -58.803799\n",
      "  69400/150000: episode: 347, duration: 1.986s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 8.605831, mae: 39.724678, mean_q: -58.907784\n",
      "  69600/150000: episode: 348, duration: 1.469s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 7.663307, mae: 39.624481, mean_q: -58.708950\n",
      "  69800/150000: episode: 349, duration: 1.447s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.156489, mae: 39.636543, mean_q: -58.519321\n",
      "  70000/150000: episode: 350, duration: 2.327s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 6.847891, mae: 39.496246, mean_q: -58.641514\n",
      "  70200/150000: episode: 351, duration: 2.346s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.735 [0.000, 2.000],  loss: 8.517716, mae: 39.505074, mean_q: -58.468826\n",
      "  70400/150000: episode: 352, duration: 2.220s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 7.843443, mae: 39.405975, mean_q: -58.376484\n",
      "  70600/150000: episode: 353, duration: 2.018s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 9.454602, mae: 39.396801, mean_q: -58.268932\n",
      "  70800/150000: episode: 354, duration: 1.690s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 5.534059, mae: 39.349262, mean_q: -58.371769\n",
      "  71000/150000: episode: 355, duration: 1.471s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.755 [0.000, 2.000],  loss: 7.105505, mae: 39.226456, mean_q: -58.038273\n",
      "  71200/150000: episode: 356, duration: 1.660s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.805 [0.000, 2.000],  loss: 10.356268, mae: 39.096123, mean_q: -57.925350\n",
      "  71400/150000: episode: 357, duration: 1.457s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 7.063304, mae: 39.045788, mean_q: -57.919247\n",
      "  71600/150000: episode: 358, duration: 1.473s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.825 [0.000, 2.000],  loss: 9.749548, mae: 38.955547, mean_q: -57.618595\n",
      "  71800/150000: episode: 359, duration: 1.735s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 8.068021, mae: 38.871895, mean_q: -57.539536\n",
      "  72000/150000: episode: 360, duration: 2.022s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 8.277873, mae: 38.857979, mean_q: -57.485695\n",
      "  72200/150000: episode: 361, duration: 1.452s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 5.625851, mae: 38.784473, mean_q: -57.507198\n",
      "  72400/150000: episode: 362, duration: 1.467s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 7.909345, mae: 38.789742, mean_q: -57.395020\n",
      "  72600/150000: episode: 363, duration: 2.118s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 6.414289, mae: 38.906734, mean_q: -57.763760\n",
      "  72800/150000: episode: 364, duration: 2.085s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 6.632397, mae: 39.041588, mean_q: -57.872589\n",
      "  73000/150000: episode: 365, duration: 2.399s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 7.381349, mae: 39.085045, mean_q: -58.021328\n",
      "  73200/150000: episode: 366, duration: 1.782s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 6.984492, mae: 39.184444, mean_q: -58.060055\n",
      "  73400/150000: episode: 367, duration: 2.014s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 5.852067, mae: 39.273975, mean_q: -58.264771\n",
      "  73600/150000: episode: 368, duration: 1.507s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 9.044827, mae: 39.406178, mean_q: -58.232979\n",
      "  73800/150000: episode: 369, duration: 2.235s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 7.721195, mae: 39.283184, mean_q: -58.240284\n",
      "  74000/150000: episode: 370, duration: 1.486s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.241734, mae: 39.353374, mean_q: -58.311447\n",
      "  74200/150000: episode: 371, duration: 1.815s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 11.650671, mae: 39.281952, mean_q: -58.147205\n",
      "  74400/150000: episode: 372, duration: 1.528s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 10.009571, mae: 39.058441, mean_q: -57.753586\n",
      "  74600/150000: episode: 373, duration: 1.972s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 7.529515, mae: 39.010380, mean_q: -57.811657\n",
      "  74800/150000: episode: 374, duration: 1.478s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.855 [0.000, 2.000],  loss: 9.146229, mae: 38.970654, mean_q: -57.699940\n",
      "  75000/150000: episode: 375, duration: 1.457s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 8.871839, mae: 38.797611, mean_q: -57.275616\n",
      "  75200/150000: episode: 376, duration: 2.114s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 8.942648, mae: 38.787148, mean_q: -57.398819\n",
      "  75400/150000: episode: 377, duration: 2.552s, episode steps: 200, steps per second:  78, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 5.913246, mae: 38.736439, mean_q: -57.489902\n",
      "  75600/150000: episode: 378, duration: 2.910s, episode steps: 200, steps per second:  69, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 7.977005, mae: 38.874725, mean_q: -57.538204\n",
      "  75800/150000: episode: 379, duration: 2.211s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 8.313953, mae: 38.825130, mean_q: -57.530190\n",
      "  76000/150000: episode: 380, duration: 1.804s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 6.542125, mae: 38.780701, mean_q: -57.566872\n",
      "  76200/150000: episode: 381, duration: 1.690s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 6.252505, mae: 38.897671, mean_q: -57.719677\n",
      "  76400/150000: episode: 382, duration: 1.655s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 8.017574, mae: 39.055019, mean_q: -57.861874\n",
      "  76600/150000: episode: 383, duration: 1.724s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 8.163117, mae: 39.070995, mean_q: -57.901867\n",
      "  76800/150000: episode: 384, duration: 2.180s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 7.890514, mae: 39.158115, mean_q: -58.011909\n",
      "  77000/150000: episode: 385, duration: 2.047s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 12.279971, mae: 39.092316, mean_q: -57.766865\n",
      "  77200/150000: episode: 386, duration: 1.511s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 9.808119, mae: 39.042793, mean_q: -57.794334\n",
      "  77400/150000: episode: 387, duration: 1.746s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 7.818439, mae: 39.042690, mean_q: -57.760502\n",
      "  77600/150000: episode: 388, duration: 1.666s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 9.076964, mae: 39.106823, mean_q: -58.004375\n",
      "  77800/150000: episode: 389, duration: 1.580s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 6.709277, mae: 39.044411, mean_q: -57.922462\n",
      "  78000/150000: episode: 390, duration: 1.765s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 9.435759, mae: 39.171558, mean_q: -58.070587\n",
      "  78200/150000: episode: 391, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 9.219968, mae: 39.176796, mean_q: -58.085812\n",
      "  78400/150000: episode: 392, duration: 1.893s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 7.055130, mae: 39.263145, mean_q: -58.233383\n",
      "  78600/150000: episode: 393, duration: 1.574s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 12.809803, mae: 39.183521, mean_q: -57.831367\n",
      "  78800/150000: episode: 394, duration: 1.632s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 7.956221, mae: 39.136906, mean_q: -58.012585\n",
      "  79000/150000: episode: 395, duration: 1.832s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 7.667207, mae: 39.188580, mean_q: -58.168182\n",
      "  79200/150000: episode: 396, duration: 2.126s, episode steps: 200, steps per second:  94, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 5.568118, mae: 39.365211, mean_q: -58.474964\n",
      "  79400/150000: episode: 397, duration: 1.474s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 10.181534, mae: 39.400276, mean_q: -58.356201\n",
      "  79600/150000: episode: 398, duration: 1.473s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 6.625652, mae: 39.439125, mean_q: -58.552025\n",
      "  79800/150000: episode: 399, duration: 2.063s, episode steps: 200, steps per second:  97, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 6.513544, mae: 39.446136, mean_q: -58.588425\n",
      "  80000/150000: episode: 400, duration: 2.047s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 8.240620, mae: 39.613827, mean_q: -58.776230\n",
      "  80200/150000: episode: 401, duration: 1.630s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.195 [0.000, 2.000],  loss: 7.649861, mae: 39.627769, mean_q: -58.678104\n",
      "  80400/150000: episode: 402, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 8.768517, mae: 39.581261, mean_q: -58.614452\n",
      "  80600/150000: episode: 403, duration: 1.479s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 7.873549, mae: 39.566105, mean_q: -58.643345\n",
      "  80800/150000: episode: 404, duration: 1.718s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 9.869469, mae: 39.520313, mean_q: -58.432030\n",
      "  81000/150000: episode: 405, duration: 1.954s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 6.944129, mae: 39.449078, mean_q: -58.611645\n",
      "  81200/150000: episode: 406, duration: 1.746s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 8.842578, mae: 39.571751, mean_q: -58.653946\n",
      "  81400/150000: episode: 407, duration: 1.452s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 9.295986, mae: 39.631855, mean_q: -58.663818\n",
      "  81600/150000: episode: 408, duration: 1.451s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 7.478682, mae: 39.590446, mean_q: -58.663700\n",
      "  81800/150000: episode: 409, duration: 2.081s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 6.975884, mae: 39.617016, mean_q: -58.817451\n",
      "  82000/150000: episode: 410, duration: 2.418s, episode steps: 200, steps per second:  83, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 8.768615, mae: 39.657707, mean_q: -58.709835\n",
      "  82200/150000: episode: 411, duration: 1.737s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 6.634431, mae: 39.649830, mean_q: -58.751801\n",
      "  82400/150000: episode: 412, duration: 2.964s, episode steps: 200, steps per second:  67, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 6.345183, mae: 39.684792, mean_q: -58.978626\n",
      "  82600/150000: episode: 413, duration: 2.380s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 6.649531, mae: 39.582901, mean_q: -58.756992\n",
      "  82800/150000: episode: 414, duration: 1.899s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 8.946150, mae: 39.548203, mean_q: -58.664879\n",
      "  83000/150000: episode: 415, duration: 1.895s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 8.247610, mae: 39.636787, mean_q: -58.755737\n",
      "  83200/150000: episode: 416, duration: 2.077s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 8.740652, mae: 39.617245, mean_q: -58.704769\n",
      "  83400/150000: episode: 417, duration: 2.388s, episode steps: 200, steps per second:  84, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 8.238482, mae: 39.489689, mean_q: -58.530205\n",
      "  83600/150000: episode: 418, duration: 2.244s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 9.368443, mae: 39.632851, mean_q: -58.730907\n",
      "  83800/150000: episode: 419, duration: 1.622s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 8.928146, mae: 39.607826, mean_q: -58.758652\n",
      "  84000/150000: episode: 420, duration: 1.928s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 7.929007, mae: 39.692291, mean_q: -58.824432\n",
      "  84200/150000: episode: 421, duration: 1.892s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 8.595342, mae: 39.683365, mean_q: -58.789345\n",
      "  84400/150000: episode: 422, duration: 1.571s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 9.936954, mae: 39.680622, mean_q: -58.709511\n",
      "  84600/150000: episode: 423, duration: 1.655s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 9.374143, mae: 39.652164, mean_q: -58.813065\n",
      "  84800/150000: episode: 424, duration: 2.246s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 7.213098, mae: 39.686535, mean_q: -58.888851\n",
      "  85000/150000: episode: 425, duration: 1.458s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 7.482693, mae: 39.764400, mean_q: -59.018974\n",
      "  85200/150000: episode: 426, duration: 1.682s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 8.250312, mae: 39.946312, mean_q: -59.104717\n",
      "  85400/150000: episode: 427, duration: 1.583s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 8.451106, mae: 39.772755, mean_q: -58.935108\n",
      "  85600/150000: episode: 428, duration: 1.507s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.755 [0.000, 2.000],  loss: 8.560656, mae: 39.662167, mean_q: -58.723358\n",
      "  85800/150000: episode: 429, duration: 1.563s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 8.567283, mae: 39.761700, mean_q: -58.928185\n",
      "  86000/150000: episode: 430, duration: 2.108s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.765 [0.000, 2.000],  loss: 11.937344, mae: 39.662846, mean_q: -58.559658\n",
      "  86200/150000: episode: 431, duration: 2.822s, episode steps: 200, steps per second:  71, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 7.671698, mae: 39.535660, mean_q: -58.624989\n",
      "  86400/150000: episode: 432, duration: 1.644s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 5.849924, mae: 39.702042, mean_q: -58.952724\n",
      "  86600/150000: episode: 433, duration: 1.755s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 10.380113, mae: 39.753204, mean_q: -58.712891\n",
      "  86800/150000: episode: 434, duration: 1.937s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 7.975152, mae: 39.760635, mean_q: -58.833656\n",
      "  87000/150000: episode: 435, duration: 2.000s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 7.619414, mae: 39.808456, mean_q: -59.018967\n",
      "  87200/150000: episode: 436, duration: 1.497s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 9.540104, mae: 39.817562, mean_q: -58.927715\n",
      "  87400/150000: episode: 437, duration: 1.875s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 9.006906, mae: 39.762573, mean_q: -58.877651\n",
      "  87600/150000: episode: 438, duration: 1.867s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 9.468559, mae: 39.844837, mean_q: -59.031055\n",
      "  87800/150000: episode: 439, duration: 1.493s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.825 [0.000, 2.000],  loss: 8.014061, mae: 39.880070, mean_q: -59.005219\n",
      "  88000/150000: episode: 440, duration: 1.639s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 6.143507, mae: 39.892120, mean_q: -59.215195\n",
      "  88200/150000: episode: 441, duration: 1.784s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 5.061157, mae: 40.046040, mean_q: -59.486290\n",
      "  88400/150000: episode: 442, duration: 2.142s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 9.738378, mae: 40.079388, mean_q: -59.317726\n",
      "  88600/150000: episode: 443, duration: 3.046s, episode steps: 200, steps per second:  66, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 8.735437, mae: 39.986752, mean_q: -59.240250\n",
      "  88800/150000: episode: 444, duration: 2.246s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 9.506726, mae: 39.994030, mean_q: -59.234463\n",
      "  89000/150000: episode: 445, duration: 1.833s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 7.423115, mae: 39.986889, mean_q: -59.340519\n",
      "  89200/150000: episode: 446, duration: 1.823s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 7.653061, mae: 40.133236, mean_q: -59.502666\n",
      "  89400/150000: episode: 447, duration: 1.780s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 8.145431, mae: 40.143776, mean_q: -59.494698\n",
      "  89600/150000: episode: 448, duration: 2.184s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 7.658992, mae: 40.126472, mean_q: -59.555908\n",
      "  89800/150000: episode: 449, duration: 2.151s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 8.718539, mae: 40.116623, mean_q: -59.359364\n",
      "  90000/150000: episode: 450, duration: 1.777s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 8.848937, mae: 39.873219, mean_q: -58.976418\n",
      "  90200/150000: episode: 451, duration: 1.716s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 9.036921, mae: 39.744663, mean_q: -58.863514\n",
      "  90400/150000: episode: 452, duration: 1.729s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 9.635155, mae: 39.523861, mean_q: -58.523163\n",
      "  90600/150000: episode: 453, duration: 1.960s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 7.165015, mae: 39.387474, mean_q: -58.374916\n",
      "  90800/150000: episode: 454, duration: 1.752s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 9.093640, mae: 39.266811, mean_q: -58.079659\n",
      "  91000/150000: episode: 455, duration: 2.494s, episode steps: 200, steps per second:  80, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 10.064868, mae: 38.899479, mean_q: -57.466503\n",
      "  91200/150000: episode: 456, duration: 2.236s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.145 [0.000, 2.000],  loss: 7.232142, mae: 38.749954, mean_q: -57.466564\n",
      "  91400/150000: episode: 457, duration: 1.791s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 9.185198, mae: 38.770592, mean_q: -57.380108\n",
      "  91600/150000: episode: 458, duration: 2.012s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 7.087681, mae: 38.716434, mean_q: -57.415352\n",
      "  91800/150000: episode: 459, duration: 2.555s, episode steps: 200, steps per second:  78, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 8.185963, mae: 38.745956, mean_q: -57.487335\n",
      "  92000/150000: episode: 460, duration: 2.193s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 10.448469, mae: 38.637836, mean_q: -57.201923\n",
      "  92200/150000: episode: 461, duration: 2.149s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 10.755575, mae: 38.514172, mean_q: -56.936424\n",
      "  92400/150000: episode: 462, duration: 2.628s, episode steps: 200, steps per second:  76, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 8.122160, mae: 38.238628, mean_q: -56.587860\n",
      "  92600/150000: episode: 463, duration: 2.262s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 7.535918, mae: 38.280304, mean_q: -56.734940\n",
      "  92800/150000: episode: 464, duration: 1.677s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 8.029459, mae: 38.207867, mean_q: -56.617950\n",
      "  93000/150000: episode: 465, duration: 1.922s, episode steps: 200, steps per second: 104, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 9.472890, mae: 38.100590, mean_q: -56.352772\n",
      "  93200/150000: episode: 466, duration: 2.360s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 9.566230, mae: 37.787338, mean_q: -55.912632\n",
      "  93400/150000: episode: 467, duration: 2.282s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.125 [0.000, 2.000],  loss: 7.692585, mae: 37.760098, mean_q: -55.950851\n",
      "  93600/150000: episode: 468, duration: 1.611s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 7.976057, mae: 37.859692, mean_q: -56.057205\n",
      "  93800/150000: episode: 469, duration: 1.956s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 5.661347, mae: 37.873802, mean_q: -56.166012\n",
      "  94000/150000: episode: 470, duration: 2.100s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 10.951045, mae: 37.863075, mean_q: -55.984081\n",
      "  94200/150000: episode: 471, duration: 1.532s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 8.190219, mae: 37.793716, mean_q: -56.033115\n",
      "  94400/150000: episode: 472, duration: 1.482s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 7.438031, mae: 37.917858, mean_q: -56.104980\n",
      "  94600/150000: episode: 473, duration: 2.196s, episode steps: 200, steps per second:  91, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 7.097182, mae: 37.834938, mean_q: -56.002735\n",
      "  94800/150000: episode: 474, duration: 1.498s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 11.118958, mae: 37.766029, mean_q: -55.834606\n",
      "  95000/150000: episode: 475, duration: 2.013s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 7.631341, mae: 37.741638, mean_q: -55.912880\n",
      "  95200/150000: episode: 476, duration: 1.482s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 8.583628, mae: 37.943153, mean_q: -56.231701\n",
      "  95400/150000: episode: 477, duration: 1.691s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 8.591548, mae: 37.865990, mean_q: -56.046425\n",
      "  95600/150000: episode: 478, duration: 2.269s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 6.606064, mae: 37.949951, mean_q: -56.241962\n",
      "  95800/150000: episode: 479, duration: 1.645s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 8.300477, mae: 37.875717, mean_q: -56.076523\n",
      "  96000/150000: episode: 480, duration: 2.794s, episode steps: 200, steps per second:  72, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 7.270585, mae: 38.031204, mean_q: -56.367695\n",
      "  96200/150000: episode: 481, duration: 2.646s, episode steps: 200, steps per second:  76, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 8.152238, mae: 38.009918, mean_q: -56.304600\n",
      "  96400/150000: episode: 482, duration: 2.161s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 8.627625, mae: 38.150986, mean_q: -56.494980\n",
      "  96600/150000: episode: 483, duration: 1.739s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 10.270025, mae: 38.093773, mean_q: -56.263641\n",
      "  96800/150000: episode: 484, duration: 1.611s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 5.748636, mae: 38.054379, mean_q: -56.419834\n",
      "  97000/150000: episode: 485, duration: 1.681s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 7.362926, mae: 38.185326, mean_q: -56.535851\n",
      "  97200/150000: episode: 486, duration: 2.032s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.785 [0.000, 2.000],  loss: 6.935987, mae: 38.127575, mean_q: -56.462334\n",
      "  97400/150000: episode: 487, duration: 1.728s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 8.233315, mae: 38.189571, mean_q: -56.511982\n",
      "  97600/150000: episode: 488, duration: 1.890s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 7.846386, mae: 38.125198, mean_q: -56.399979\n",
      "  97800/150000: episode: 489, duration: 1.828s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 7.626040, mae: 38.213661, mean_q: -56.569103\n",
      "  98000/150000: episode: 490, duration: 2.001s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 7.612471, mae: 38.112274, mean_q: -56.367588\n",
      "  98200/150000: episode: 491, duration: 1.797s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 7.676119, mae: 38.042915, mean_q: -56.335880\n",
      "  98400/150000: episode: 492, duration: 1.856s, episode steps: 200, steps per second: 108, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 6.067897, mae: 37.977978, mean_q: -56.277412\n",
      "  98600/150000: episode: 493, duration: 1.508s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 5.180394, mae: 37.985363, mean_q: -56.268829\n",
      "  98800/150000: episode: 494, duration: 1.863s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 8.695432, mae: 38.032902, mean_q: -56.260635\n",
      "  99000/150000: episode: 495, duration: 1.646s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 5.490005, mae: 37.934139, mean_q: -56.209141\n",
      "  99200/150000: episode: 496, duration: 1.449s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 6.398109, mae: 38.016178, mean_q: -56.302193\n",
      "  99400/150000: episode: 497, duration: 1.463s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 8.637289, mae: 37.923374, mean_q: -56.072121\n",
      "  99600/150000: episode: 498, duration: 1.444s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 7.322001, mae: 38.062656, mean_q: -56.392960\n",
      "  99800/150000: episode: 499, duration: 1.456s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 11.274978, mae: 37.847775, mean_q: -55.769508\n",
      " 100000/150000: episode: 500, duration: 1.668s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 7.241709, mae: 37.696274, mean_q: -55.809364\n",
      " 100200/150000: episode: 501, duration: 1.579s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 7.234698, mae: 37.770756, mean_q: -55.777042\n",
      " 100400/150000: episode: 502, duration: 1.665s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 7.405424, mae: 37.678417, mean_q: -55.726791\n",
      " 100600/150000: episode: 503, duration: 1.627s, episode steps: 200, steps per second: 123, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.100 [0.000, 2.000],  loss: 7.774955, mae: 37.709824, mean_q: -55.757687\n",
      " 100800/150000: episode: 504, duration: 1.961s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 6.920462, mae: 37.563065, mean_q: -55.552315\n",
      " 101000/150000: episode: 505, duration: 1.599s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 6.015765, mae: 37.441891, mean_q: -55.435619\n",
      " 101200/150000: episode: 506, duration: 1.658s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 6.071365, mae: 37.447826, mean_q: -55.372116\n",
      " 101400/150000: episode: 507, duration: 1.800s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 7.402788, mae: 37.484692, mean_q: -55.367176\n",
      " 101600/150000: episode: 508, duration: 1.707s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.360 [0.000, 2.000],  loss: 8.924734, mae: 37.134472, mean_q: -54.727901\n",
      " 101800/150000: episode: 509, duration: 1.685s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.250 [0.000, 2.000],  loss: 4.905334, mae: 37.032707, mean_q: -54.707352\n",
      " 102000/150000: episode: 510, duration: 1.646s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.325 [0.000, 2.000],  loss: 8.053705, mae: 36.897202, mean_q: -54.403339\n",
      " 102200/150000: episode: 511, duration: 1.885s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.255 [0.000, 2.000],  loss: 6.828414, mae: 36.400318, mean_q: -53.554169\n",
      " 102400/150000: episode: 512, duration: 1.660s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.400 [0.000, 2.000],  loss: 6.101660, mae: 35.920357, mean_q: -52.882851\n",
      " 102600/150000: episode: 513, duration: 2.092s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.275 [0.000, 2.000],  loss: 5.161594, mae: 35.633446, mean_q: -52.607853\n",
      " 102800/150000: episode: 514, duration: 2.675s, episode steps: 200, steps per second:  75, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.295 [0.000, 2.000],  loss: 6.618469, mae: 35.432663, mean_q: -52.170303\n",
      " 103000/150000: episode: 515, duration: 3.058s, episode steps: 200, steps per second:  65, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.380 [0.000, 2.000],  loss: 5.301177, mae: 35.199093, mean_q: -51.840164\n",
      " 103200/150000: episode: 516, duration: 2.345s, episode steps: 200, steps per second:  85, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.460 [0.000, 2.000],  loss: 5.366365, mae: 34.670391, mean_q: -50.975292\n",
      " 103400/150000: episode: 517, duration: 3.188s, episode steps: 200, steps per second:  63, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.415 [0.000, 2.000],  loss: 4.544124, mae: 34.326870, mean_q: -50.507565\n",
      " 103561/150000: episode: 518, duration: 2.774s, episode steps: 161, steps per second:  58, episode reward: -161.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.304 [0.000, 2.000],  loss: 4.129397, mae: 33.826733, mean_q: -49.714317\n",
      " 103761/150000: episode: 519, duration: 2.723s, episode steps: 200, steps per second:  73, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.240 [0.000, 2.000],  loss: 5.364129, mae: 33.615585, mean_q: -49.408478\n",
      " 103961/150000: episode: 520, duration: 2.051s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.400 [0.000, 2.000],  loss: 6.099627, mae: 33.116844, mean_q: -48.522499\n",
      " 104161/150000: episode: 521, duration: 3.159s, episode steps: 200, steps per second:  63, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.205 [0.000, 2.000],  loss: 4.389874, mae: 32.626476, mean_q: -47.916630\n",
      " 104361/150000: episode: 522, duration: 2.328s, episode steps: 200, steps per second:  86, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.315 [0.000, 2.000],  loss: 5.572248, mae: 32.341465, mean_q: -47.440777\n",
      " 104561/150000: episode: 523, duration: 1.466s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.175 [0.000, 2.000],  loss: 5.663045, mae: 31.784863, mean_q: -46.599625\n",
      " 104761/150000: episode: 524, duration: 1.458s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.095 [0.000, 2.000],  loss: 5.226826, mae: 31.399214, mean_q: -46.064838\n",
      " 104961/150000: episode: 525, duration: 1.520s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.160 [0.000, 2.000],  loss: 3.593781, mae: 31.262329, mean_q: -45.881035\n",
      " 105161/150000: episode: 526, duration: 1.654s, episode steps: 200, steps per second: 121, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.170 [0.000, 2.000],  loss: 3.479715, mae: 31.077675, mean_q: -45.709400\n",
      " 105361/150000: episode: 527, duration: 1.664s, episode steps: 200, steps per second: 120, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 3.401551, mae: 30.816217, mean_q: -45.361954\n",
      " 105561/150000: episode: 528, duration: 1.699s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 5.641783, mae: 30.584732, mean_q: -44.866856\n",
      " 105761/150000: episode: 529, duration: 1.693s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 4.409587, mae: 30.223700, mean_q: -44.436035\n",
      " 105961/150000: episode: 530, duration: 1.539s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 4.573740, mae: 30.239782, mean_q: -44.439404\n",
      " 106161/150000: episode: 531, duration: 1.697s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 3.962137, mae: 30.180489, mean_q: -44.343704\n",
      " 106361/150000: episode: 532, duration: 1.878s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 4.281413, mae: 29.950647, mean_q: -43.943344\n",
      " 106561/150000: episode: 533, duration: 1.937s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 3.513825, mae: 29.588621, mean_q: -43.451637\n",
      " 106761/150000: episode: 534, duration: 2.268s, episode steps: 200, steps per second:  88, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 3.392263, mae: 29.603443, mean_q: -43.468597\n",
      " 106961/150000: episode: 535, duration: 1.808s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 3.857414, mae: 29.414989, mean_q: -43.118446\n",
      " 107161/150000: episode: 536, duration: 1.515s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 4.404746, mae: 29.164345, mean_q: -42.733311\n",
      " 107361/150000: episode: 537, duration: 1.993s, episode steps: 200, steps per second: 100, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.810 [0.000, 2.000],  loss: 2.076478, mae: 28.976978, mean_q: -42.612438\n",
      " 107561/150000: episode: 538, duration: 2.093s, episode steps: 200, steps per second:  96, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 3.973955, mae: 28.983047, mean_q: -42.512104\n",
      " 107761/150000: episode: 539, duration: 2.226s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 4.436040, mae: 28.931812, mean_q: -42.414783\n",
      " 107961/150000: episode: 540, duration: 2.047s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 4.536245, mae: 28.734707, mean_q: -42.110348\n",
      " 108161/150000: episode: 541, duration: 2.311s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 4.264495, mae: 28.517935, mean_q: -41.849052\n",
      " 108361/150000: episode: 542, duration: 2.222s, episode steps: 200, steps per second:  90, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 5.632381, mae: 28.536909, mean_q: -41.828243\n",
      " 108561/150000: episode: 543, duration: 2.184s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 2.863755, mae: 28.499580, mean_q: -41.954151\n",
      " 108761/150000: episode: 544, duration: 2.177s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 3.124420, mae: 28.604362, mean_q: -42.086288\n",
      " 108961/150000: episode: 545, duration: 1.527s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 3.846443, mae: 28.480219, mean_q: -41.801826\n",
      " 109161/150000: episode: 546, duration: 1.434s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 4.506193, mae: 28.490234, mean_q: -41.825539\n",
      " 109361/150000: episode: 547, duration: 1.413s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 3.805858, mae: 28.163773, mean_q: -41.261322\n",
      " 109561/150000: episode: 548, duration: 1.418s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 3.456984, mae: 28.108692, mean_q: -41.231598\n",
      " 109761/150000: episode: 549, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.210 [0.000, 2.000],  loss: 3.361498, mae: 27.979809, mean_q: -40.997101\n",
      " 109961/150000: episode: 550, duration: 1.441s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 3.347135, mae: 27.756315, mean_q: -40.591991\n",
      " 110161/150000: episode: 551, duration: 1.692s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 3.497786, mae: 27.355946, mean_q: -39.972523\n",
      " 110361/150000: episode: 552, duration: 1.426s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 4.275092, mae: 27.036221, mean_q: -39.408340\n",
      " 110561/150000: episode: 553, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 2.980560, mae: 26.448204, mean_q: -38.548447\n",
      " 110761/150000: episode: 554, duration: 1.404s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.230 [0.000, 2.000],  loss: 3.471410, mae: 26.164139, mean_q: -38.170048\n",
      " 110961/150000: episode: 555, duration: 1.570s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 3.126984, mae: 25.810503, mean_q: -37.672241\n",
      " 111161/150000: episode: 556, duration: 1.636s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.185 [0.000, 2.000],  loss: 2.439814, mae: 25.377548, mean_q: -37.110699\n",
      " 111361/150000: episode: 557, duration: 2.027s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 2.125347, mae: 25.149443, mean_q: -36.799362\n",
      " 111561/150000: episode: 558, duration: 2.480s, episode steps: 200, steps per second:  81, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 2.802053, mae: 25.116375, mean_q: -36.769615\n",
      " 111761/150000: episode: 559, duration: 1.491s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.190 [0.000, 2.000],  loss: 2.321542, mae: 24.811874, mean_q: -36.366161\n",
      " 111961/150000: episode: 560, duration: 1.557s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 4.081232, mae: 24.682955, mean_q: -36.018978\n",
      " 112161/150000: episode: 561, duration: 1.947s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 3.127348, mae: 24.694429, mean_q: -36.264854\n",
      " 112361/150000: episode: 562, duration: 1.450s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 2.432938, mae: 24.778738, mean_q: -36.450424\n",
      " 112561/150000: episode: 563, duration: 1.798s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 2.484707, mae: 24.819725, mean_q: -36.474518\n",
      " 112761/150000: episode: 564, duration: 1.491s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 2.881224, mae: 24.885927, mean_q: -36.613731\n",
      " 112961/150000: episode: 565, duration: 1.754s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 2.321068, mae: 25.139946, mean_q: -36.970203\n",
      " 113161/150000: episode: 566, duration: 1.428s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 1.735346, mae: 25.398447, mean_q: -37.377705\n",
      " 113361/150000: episode: 567, duration: 1.800s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.835 [0.000, 2.000],  loss: 2.352101, mae: 25.482962, mean_q: -37.403931\n",
      " 113561/150000: episode: 568, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 2.911934, mae: 25.461359, mean_q: -37.320168\n",
      " 113761/150000: episode: 569, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 2.920249, mae: 25.437019, mean_q: -37.319595\n",
      " 113961/150000: episode: 570, duration: 2.303s, episode steps: 200, steps per second:  87, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.770 [0.000, 2.000],  loss: 2.118764, mae: 25.417852, mean_q: -37.269264\n",
      " 114161/150000: episode: 571, duration: 2.029s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 2.568394, mae: 25.460081, mean_q: -37.357651\n",
      " 114361/150000: episode: 572, duration: 2.021s, episode steps: 200, steps per second:  99, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.805 [0.000, 2.000],  loss: 4.153143, mae: 25.529678, mean_q: -37.386578\n",
      " 114561/150000: episode: 573, duration: 1.889s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.790 [0.000, 2.000],  loss: 3.342597, mae: 25.459053, mean_q: -37.341232\n",
      " 114761/150000: episode: 574, duration: 1.971s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.830 [0.000, 2.000],  loss: 2.536655, mae: 25.476658, mean_q: -37.410412\n",
      " 114961/150000: episode: 575, duration: 1.949s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 2.357806, mae: 25.519270, mean_q: -37.546227\n",
      " 115161/150000: episode: 576, duration: 2.248s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 4.588258, mae: 25.591259, mean_q: -37.541092\n",
      " 115361/150000: episode: 577, duration: 2.147s, episode steps: 200, steps per second:  93, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 2.568582, mae: 25.608192, mean_q: -37.657249\n",
      " 115561/150000: episode: 578, duration: 2.045s, episode steps: 200, steps per second:  98, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 2.778769, mae: 25.708712, mean_q: -37.864159\n",
      " 115761/150000: episode: 579, duration: 1.899s, episode steps: 200, steps per second: 105, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 4.012294, mae: 25.934135, mean_q: -38.196823\n",
      " 115961/150000: episode: 580, duration: 2.978s, episode steps: 200, steps per second:  67, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 3.431542, mae: 26.067377, mean_q: -38.379902\n",
      " 116161/150000: episode: 581, duration: 2.165s, episode steps: 200, steps per second:  92, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 3.163513, mae: 26.263788, mean_q: -38.666676\n",
      " 116361/150000: episode: 582, duration: 1.569s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 4.404925, mae: 26.332165, mean_q: -38.683815\n",
      " 116561/150000: episode: 583, duration: 2.427s, episode steps: 200, steps per second:  82, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 2.885050, mae: 26.485668, mean_q: -38.995804\n",
      " 116761/150000: episode: 584, duration: 1.967s, episode steps: 200, steps per second: 102, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 3.731626, mae: 26.370066, mean_q: -38.727749\n",
      " 116961/150000: episode: 585, duration: 1.989s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.155 [0.000, 2.000],  loss: 2.782970, mae: 26.610342, mean_q: -39.201088\n",
      " 117161/150000: episode: 586, duration: 1.766s, episode steps: 200, steps per second: 113, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 3.338746, mae: 26.614460, mean_q: -39.075821\n",
      " 117361/150000: episode: 587, duration: 2.235s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000],  loss: 2.946874, mae: 26.607882, mean_q: -39.038090\n",
      " 117561/150000: episode: 588, duration: 1.751s, episode steps: 200, steps per second: 114, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 2.791265, mae: 26.384426, mean_q: -38.761173\n",
      " 117761/150000: episode: 589, duration: 1.703s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.215 [0.000, 2.000],  loss: 3.198023, mae: 26.342463, mean_q: -38.605011\n",
      " 117961/150000: episode: 590, duration: 1.618s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.150 [0.000, 2.000],  loss: 2.466964, mae: 26.142937, mean_q: -38.389713\n",
      " 118161/150000: episode: 591, duration: 1.787s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 2.923771, mae: 26.316042, mean_q: -38.651917\n",
      " 118361/150000: episode: 592, duration: 1.636s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 2.860722, mae: 26.123720, mean_q: -38.355392\n",
      " 118561/150000: episode: 593, duration: 1.800s, episode steps: 200, steps per second: 111, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 2.522511, mae: 26.079727, mean_q: -38.320522\n",
      " 118761/150000: episode: 594, duration: 1.832s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.165 [0.000, 2.000],  loss: 3.908242, mae: 26.109669, mean_q: -38.299030\n",
      " 118961/150000: episode: 595, duration: 1.605s, episode steps: 200, steps per second: 125, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 3.711139, mae: 26.136337, mean_q: -38.308704\n",
      " 119161/150000: episode: 596, duration: 1.506s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 2.856111, mae: 25.890869, mean_q: -37.971458\n",
      " 119361/150000: episode: 597, duration: 1.883s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.090 [0.000, 2.000],  loss: 2.306602, mae: 25.862831, mean_q: -38.101757\n",
      " 119561/150000: episode: 598, duration: 1.833s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 1.717024, mae: 26.093208, mean_q: -38.478634\n",
      " 119761/150000: episode: 599, duration: 1.716s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 3.235562, mae: 26.346886, mean_q: -38.771347\n",
      " 119961/150000: episode: 600, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.070 [0.000, 2.000],  loss: 3.059643, mae: 26.330194, mean_q: -38.765427\n",
      " 120161/150000: episode: 601, duration: 1.551s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.025 [0.000, 2.000],  loss: 2.963626, mae: 26.491142, mean_q: -39.032070\n",
      " 120361/150000: episode: 602, duration: 1.707s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 3.237676, mae: 26.611187, mean_q: -39.166889\n",
      " 120561/150000: episode: 603, duration: 1.557s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.586942, mae: 26.612671, mean_q: -39.214611\n",
      " 120761/150000: episode: 604, duration: 1.534s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 2.525345, mae: 26.752642, mean_q: -39.417648\n",
      " 120961/150000: episode: 605, duration: 1.942s, episode steps: 200, steps per second: 103, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 3.243668, mae: 26.800400, mean_q: -39.488926\n",
      " 121161/150000: episode: 606, duration: 2.237s, episode steps: 200, steps per second:  89, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 2.170120, mae: 26.946518, mean_q: -39.763882\n",
      " 121361/150000: episode: 607, duration: 2.597s, episode steps: 200, steps per second:  77, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 3.104073, mae: 27.137413, mean_q: -39.985470\n",
      " 121561/150000: episode: 608, duration: 1.720s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 2.310570, mae: 27.254929, mean_q: -40.162407\n",
      " 121761/150000: episode: 609, duration: 1.861s, episode steps: 200, steps per second: 107, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 3.114105, mae: 27.267078, mean_q: -40.177204\n",
      " 121961/150000: episode: 610, duration: 1.475s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.875 [0.000, 2.000],  loss: 2.446157, mae: 27.392597, mean_q: -40.400562\n",
      " 122161/150000: episode: 611, duration: 1.583s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.720 [0.000, 2.000],  loss: 3.020010, mae: 27.493584, mean_q: -40.487770\n",
      " 122361/150000: episode: 612, duration: 1.500s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 3.008459, mae: 27.599560, mean_q: -40.666763\n",
      " 122561/150000: episode: 613, duration: 1.555s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 4.101657, mae: 27.513889, mean_q: -40.481232\n",
      " 122761/150000: episode: 614, duration: 1.501s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 2.739172, mae: 27.782425, mean_q: -40.997086\n",
      " 122961/150000: episode: 615, duration: 1.455s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.795 [0.000, 2.000],  loss: 3.682175, mae: 27.802280, mean_q: -40.994610\n",
      " 123161/150000: episode: 616, duration: 1.518s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 3.142159, mae: 28.014814, mean_q: -41.358868\n",
      " 123361/150000: episode: 617, duration: 1.558s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 3.888751, mae: 28.131950, mean_q: -41.513073\n",
      " 123561/150000: episode: 618, duration: 2.535s, episode steps: 200, steps per second:  79, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 3.221253, mae: 28.261597, mean_q: -41.647755\n",
      " 123761/150000: episode: 619, duration: 2.662s, episode steps: 200, steps per second:  75, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 3.801254, mae: 28.323008, mean_q: -41.815899\n",
      " 123961/150000: episode: 620, duration: 2.096s, episode steps: 200, steps per second:  95, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 5.076725, mae: 28.326698, mean_q: -41.715897\n",
      " 124161/150000: episode: 621, duration: 1.482s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 3.663666, mae: 28.488981, mean_q: -42.014294\n",
      " 124361/150000: episode: 622, duration: 1.786s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 2.590864, mae: 28.649248, mean_q: -42.318478\n",
      " 124561/150000: episode: 623, duration: 1.498s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 3.095441, mae: 28.685688, mean_q: -42.249523\n",
      " 124761/150000: episode: 624, duration: 1.550s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 3.599767, mae: 28.562489, mean_q: -42.142006\n",
      " 124961/150000: episode: 625, duration: 1.513s, episode steps: 200, steps per second: 132, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 3.322991, mae: 28.782204, mean_q: -42.455063\n",
      " 125161/150000: episode: 626, duration: 1.839s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 3.537503, mae: 28.855347, mean_q: -42.569592\n",
      " 125361/150000: episode: 627, duration: 1.730s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 3.409043, mae: 28.922352, mean_q: -42.685528\n",
      " 125561/150000: episode: 628, duration: 1.506s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 3.709153, mae: 29.040085, mean_q: -42.844505\n",
      " 125761/150000: episode: 629, duration: 1.982s, episode steps: 200, steps per second: 101, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 3.656607, mae: 29.019991, mean_q: -42.870548\n",
      " 125961/150000: episode: 630, duration: 1.530s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.135 [0.000, 2.000],  loss: 3.260559, mae: 29.218365, mean_q: -43.186310\n",
      " 126161/150000: episode: 631, duration: 1.502s, episode steps: 200, steps per second: 133, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 3.432274, mae: 29.301626, mean_q: -43.278393\n",
      " 126361/150000: episode: 632, duration: 1.702s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 3.710818, mae: 29.389166, mean_q: -43.360031\n",
      " 126561/150000: episode: 633, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 4.146983, mae: 29.493862, mean_q: -43.424759\n",
      " 126761/150000: episode: 634, duration: 1.440s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 3.569561, mae: 29.444355, mean_q: -43.433868\n",
      " 126961/150000: episode: 635, duration: 1.691s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.110 [0.000, 2.000],  loss: 3.470565, mae: 29.578867, mean_q: -43.634785\n",
      " 127161/150000: episode: 636, duration: 1.576s, episode steps: 200, steps per second: 127, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 3.525558, mae: 29.424126, mean_q: -43.376942\n",
      " 127361/150000: episode: 637, duration: 1.530s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 3.380022, mae: 29.643837, mean_q: -43.703739\n",
      " 127561/150000: episode: 638, duration: 1.498s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 3.538254, mae: 29.475056, mean_q: -43.467213\n",
      " 127761/150000: episode: 639, duration: 1.475s, episode steps: 200, steps per second: 136, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 2.590492, mae: 29.705795, mean_q: -43.856552\n",
      " 127961/150000: episode: 640, duration: 1.525s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 3.357525, mae: 29.646528, mean_q: -43.725830\n",
      " 128161/150000: episode: 641, duration: 1.704s, episode steps: 200, steps per second: 117, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 3.956400, mae: 29.720913, mean_q: -43.736210\n",
      " 128361/150000: episode: 642, duration: 1.784s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 2.841426, mae: 29.603334, mean_q: -43.660255\n",
      " 128561/150000: episode: 643, duration: 1.677s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 3.191134, mae: 29.632418, mean_q: -43.657520\n",
      " 128761/150000: episode: 644, duration: 1.444s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 4.052515, mae: 29.609791, mean_q: -43.519947\n",
      " 128961/150000: episode: 645, duration: 1.736s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.080 [0.000, 2.000],  loss: 3.039316, mae: 29.346870, mean_q: -43.227711\n",
      " 129161/150000: episode: 646, duration: 1.743s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 3.393585, mae: 29.359146, mean_q: -43.185230\n",
      " 129361/150000: episode: 647, duration: 1.544s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 2.920654, mae: 29.437000, mean_q: -43.401314\n",
      " 129561/150000: episode: 648, duration: 1.535s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 2.800522, mae: 29.431128, mean_q: -43.404453\n",
      " 129761/150000: episode: 649, duration: 1.530s, episode steps: 200, steps per second: 131, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 3.994395, mae: 29.370934, mean_q: -43.198887\n",
      " 129961/150000: episode: 650, duration: 1.533s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 3.198339, mae: 29.469135, mean_q: -43.438465\n",
      " 130161/150000: episode: 651, duration: 1.684s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.870 [0.000, 2.000],  loss: 2.674747, mae: 29.492138, mean_q: -43.521046\n",
      " 130361/150000: episode: 652, duration: 1.615s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 4.277763, mae: 29.524389, mean_q: -43.489822\n",
      " 130561/150000: episode: 653, duration: 1.537s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 4.469805, mae: 29.310556, mean_q: -43.157902\n",
      " 130761/150000: episode: 654, duration: 1.540s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 2.601635, mae: 29.374975, mean_q: -43.392433\n",
      " 130961/150000: episode: 655, duration: 1.535s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 1.707330, mae: 29.458790, mean_q: -43.554913\n",
      " 131161/150000: episode: 656, duration: 1.539s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 3.398218, mae: 29.782997, mean_q: -43.935020\n",
      " 131361/150000: episode: 657, duration: 1.558s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 4.572029, mae: 29.656555, mean_q: -43.717979\n",
      " 131561/150000: episode: 658, duration: 1.541s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 2.663530, mae: 29.753944, mean_q: -43.972656\n",
      " 131761/150000: episode: 659, duration: 1.541s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.930 [0.000, 2.000],  loss: 3.757371, mae: 29.892666, mean_q: -44.067333\n",
      " 131961/150000: episode: 660, duration: 1.550s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 3.399552, mae: 29.983921, mean_q: -44.232101\n",
      " 132161/150000: episode: 661, duration: 1.546s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 3.635587, mae: 29.856142, mean_q: -44.013206\n",
      " 132361/150000: episode: 662, duration: 1.583s, episode steps: 200, steps per second: 126, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 3.660177, mae: 29.829605, mean_q: -43.982689\n",
      " 132561/150000: episode: 663, duration: 1.639s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 2.077897, mae: 29.927620, mean_q: -44.198990\n",
      " 132761/150000: episode: 664, duration: 1.817s, episode steps: 200, steps per second: 110, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.105 [0.000, 2.000],  loss: 3.707820, mae: 29.890430, mean_q: -44.075085\n",
      " 132961/150000: episode: 665, duration: 1.546s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 3.151544, mae: 30.026875, mean_q: -44.282280\n",
      " 133161/150000: episode: 666, duration: 1.535s, episode steps: 200, steps per second: 130, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 2.176711, mae: 30.149155, mean_q: -44.490021\n",
      " 133361/150000: episode: 667, duration: 1.546s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 3.420165, mae: 30.020546, mean_q: -44.196568\n",
      " 133559/150000: episode: 668, duration: 1.924s, episode steps: 198, steps per second: 103, episode reward: -198.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.146 [0.000, 2.000],  loss: 4.024360, mae: 29.857727, mean_q: -43.890522\n",
      " 133759/150000: episode: 669, duration: 1.612s, episode steps: 200, steps per second: 124, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 3.778454, mae: 30.050360, mean_q: -44.262192\n",
      " 133959/150000: episode: 670, duration: 1.878s, episode steps: 200, steps per second: 106, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 2.555391, mae: 30.000025, mean_q: -44.220871\n",
      " 134159/150000: episode: 671, duration: 1.835s, episode steps: 200, steps per second: 109, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 4.229269, mae: 30.048454, mean_q: -44.254604\n",
      " 134359/150000: episode: 672, duration: 1.780s, episode steps: 200, steps per second: 112, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 2.364857, mae: 29.930334, mean_q: -44.190296\n",
      " 134559/150000: episode: 673, duration: 1.738s, episode steps: 200, steps per second: 115, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.140 [0.000, 2.000],  loss: 2.807489, mae: 29.931206, mean_q: -44.097797\n",
      " 134759/150000: episode: 674, duration: 1.677s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.840 [0.000, 2.000],  loss: 4.624061, mae: 29.810230, mean_q: -43.788517\n",
      " 134959/150000: episode: 675, duration: 1.568s, episode steps: 200, steps per second: 128, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.850 [0.000, 2.000],  loss: 3.804772, mae: 29.687305, mean_q: -43.722046\n",
      " 135159/150000: episode: 676, duration: 1.730s, episode steps: 200, steps per second: 116, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 3.888484, mae: 29.582983, mean_q: -43.588078\n",
      " 135359/150000: episode: 677, duration: 1.429s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 2.839226, mae: 29.616819, mean_q: -43.677891\n",
      " 135559/150000: episode: 678, duration: 1.448s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 3.740478, mae: 29.509577, mean_q: -43.432693\n",
      " 135759/150000: episode: 679, duration: 1.428s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.860 [0.000, 2.000],  loss: 2.561904, mae: 29.706919, mean_q: -43.808205\n",
      " 135959/150000: episode: 680, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.902740, mae: 29.815718, mean_q: -43.978848\n",
      " 136159/150000: episode: 681, duration: 1.415s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.905 [0.000, 2.000],  loss: 2.882481, mae: 29.709541, mean_q: -43.855434\n",
      " 136359/150000: episode: 682, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 4.036705, mae: 29.737865, mean_q: -43.832783\n",
      " 136559/150000: episode: 683, duration: 1.431s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 2.751169, mae: 29.833521, mean_q: -44.040024\n",
      " 136759/150000: episode: 684, duration: 1.405s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 2.654317, mae: 30.048950, mean_q: -44.403507\n",
      " 136959/150000: episode: 685, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 2.608318, mae: 29.924761, mean_q: -44.175884\n",
      " 137159/150000: episode: 686, duration: 1.432s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 3.266947, mae: 29.990061, mean_q: -44.213890\n",
      " 137359/150000: episode: 687, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 2.646536, mae: 30.192617, mean_q: -44.551491\n",
      " 137559/150000: episode: 688, duration: 1.439s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.035 [0.000, 2.000],  loss: 2.307438, mae: 30.195503, mean_q: -44.574207\n",
      " 137759/150000: episode: 689, duration: 1.407s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.010 [0.000, 2.000],  loss: 2.962131, mae: 30.272154, mean_q: -44.650063\n",
      " 137959/150000: episode: 690, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.965 [0.000, 2.000],  loss: 3.535518, mae: 30.138252, mean_q: -44.355751\n",
      " 138159/150000: episode: 691, duration: 1.405s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 2.638968, mae: 30.067810, mean_q: -44.258141\n",
      " 138359/150000: episode: 692, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.990 [0.000, 2.000],  loss: 3.416471, mae: 29.907192, mean_q: -43.971161\n",
      " 138559/150000: episode: 693, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 2.024188, mae: 29.877796, mean_q: -43.982834\n",
      " 138759/150000: episode: 694, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 2.218828, mae: 29.707651, mean_q: -43.733051\n",
      " 138959/150000: episode: 695, duration: 1.411s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 4.035263, mae: 29.584011, mean_q: -43.447826\n",
      " 139159/150000: episode: 696, duration: 1.407s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 3.457344, mae: 29.276461, mean_q: -42.997482\n",
      " 139359/150000: episode: 697, duration: 1.403s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.130 [0.000, 2.000],  loss: 2.483419, mae: 29.086315, mean_q: -42.806202\n",
      " 139559/150000: episode: 698, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.120 [0.000, 2.000],  loss: 3.259336, mae: 29.220121, mean_q: -42.943867\n",
      " 139759/150000: episode: 699, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 2.858413, mae: 28.817158, mean_q: -42.357666\n",
      " 139959/150000: episode: 700, duration: 1.413s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.955 [0.000, 2.000],  loss: 2.836676, mae: 28.724403, mean_q: -42.254623\n",
      " 140159/150000: episode: 701, duration: 1.405s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.995 [0.000, 2.000],  loss: 2.775705, mae: 28.652672, mean_q: -42.123154\n",
      " 140359/150000: episode: 702, duration: 1.411s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 3.522998, mae: 28.517143, mean_q: -41.921719\n",
      " 140559/150000: episode: 703, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 1.666274, mae: 28.677681, mean_q: -42.254196\n",
      " 140759/150000: episode: 704, duration: 1.404s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.040 [0.000, 2.000],  loss: 2.213594, mae: 28.691246, mean_q: -42.291767\n",
      " 140959/150000: episode: 705, duration: 1.409s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.960 [0.000, 2.000],  loss: 3.037806, mae: 28.778662, mean_q: -42.395294\n",
      " 141159/150000: episode: 706, duration: 1.402s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.975 [0.000, 2.000],  loss: 2.923593, mae: 28.638565, mean_q: -42.129120\n",
      " 141359/150000: episode: 707, duration: 1.411s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 2.608116, mae: 28.533003, mean_q: -41.982533\n",
      " 141559/150000: episode: 708, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.925 [0.000, 2.000],  loss: 3.964333, mae: 28.679804, mean_q: -42.152088\n",
      " 141759/150000: episode: 709, duration: 1.409s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.005 [0.000, 2.000],  loss: 3.081981, mae: 28.477114, mean_q: -41.888260\n",
      " 141959/150000: episode: 710, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.915 [0.000, 2.000],  loss: 2.914323, mae: 28.453238, mean_q: -41.867966\n",
      " 142159/150000: episode: 711, duration: 1.407s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 2.564381, mae: 28.555159, mean_q: -42.039597\n",
      " 142359/150000: episode: 712, duration: 1.405s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 2.449269, mae: 28.558262, mean_q: -42.012089\n",
      " 142559/150000: episode: 713, duration: 1.401s, episode steps: 200, steps per second: 143, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 2.286119, mae: 28.321976, mean_q: -41.688702\n",
      " 142759/150000: episode: 714, duration: 1.437s, episode steps: 200, steps per second: 139, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.055 [0.000, 2.000],  loss: 2.278388, mae: 28.359409, mean_q: -41.724312\n",
      " 142959/150000: episode: 715, duration: 1.405s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 2.531062, mae: 28.288734, mean_q: -41.565582\n",
      " 143121/150000: episode: 716, duration: 1.141s, episode steps: 162, steps per second: 142, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.012 [0.000, 2.000],  loss: 1.649184, mae: 28.070251, mean_q: -41.281315\n",
      " 143321/150000: episode: 717, duration: 1.421s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.085 [0.000, 2.000],  loss: 2.142434, mae: 28.187090, mean_q: -41.386944\n",
      " 143521/150000: episode: 718, duration: 1.413s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.945 [0.000, 2.000],  loss: 2.258329, mae: 28.112055, mean_q: -41.290596\n",
      " 143721/150000: episode: 719, duration: 1.424s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.820 [0.000, 2.000],  loss: 2.925201, mae: 27.813686, mean_q: -40.780373\n",
      " 143921/150000: episode: 720, duration: 1.418s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 2.472570, mae: 27.845436, mean_q: -40.814377\n",
      " 144121/150000: episode: 721, duration: 1.463s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.885 [0.000, 2.000],  loss: 2.249785, mae: 27.675007, mean_q: -40.654198\n",
      " 144321/150000: episode: 722, duration: 1.701s, episode steps: 200, steps per second: 118, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.015 [0.000, 2.000],  loss: 2.250734, mae: 27.547321, mean_q: -40.444561\n",
      " 144521/150000: episode: 723, duration: 1.552s, episode steps: 200, steps per second: 129, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.880 [0.000, 2.000],  loss: 1.499067, mae: 27.510958, mean_q: -40.516197\n",
      " 144721/150000: episode: 724, duration: 1.460s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.030 [0.000, 2.000],  loss: 2.491729, mae: 27.531382, mean_q: -40.432384\n",
      " 144921/150000: episode: 725, duration: 1.407s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.900 [0.000, 2.000],  loss: 1.463655, mae: 27.487757, mean_q: -40.500751\n",
      " 145121/150000: episode: 726, duration: 1.420s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.895 [0.000, 2.000],  loss: 2.099309, mae: 27.517197, mean_q: -40.433632\n",
      " 145321/150000: episode: 727, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 2.218446, mae: 27.375366, mean_q: -40.229530\n",
      " 145521/150000: episode: 728, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.940 [0.000, 2.000],  loss: 2.610099, mae: 27.352013, mean_q: -40.213062\n",
      " 145721/150000: episode: 729, duration: 1.636s, episode steps: 200, steps per second: 122, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 1.999429, mae: 27.384636, mean_q: -40.231495\n",
      " 145921/150000: episode: 730, duration: 1.678s, episode steps: 200, steps per second: 119, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.020 [0.000, 2.000],  loss: 2.549317, mae: 27.515114, mean_q: -40.441902\n",
      " 146121/150000: episode: 731, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.060 [0.000, 2.000],  loss: 2.208441, mae: 27.295948, mean_q: -40.139168\n",
      " 146321/150000: episode: 732, duration: 1.419s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.075 [0.000, 2.000],  loss: 1.860007, mae: 27.362988, mean_q: -40.289906\n",
      " 146521/150000: episode: 733, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.890 [0.000, 2.000],  loss: 1.691928, mae: 27.296675, mean_q: -40.208061\n",
      " 146721/150000: episode: 734, duration: 1.426s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.050 [0.000, 2.000],  loss: 1.945688, mae: 27.413074, mean_q: -40.259186\n",
      " 146921/150000: episode: 735, duration: 1.426s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.115 [0.000, 2.000],  loss: 2.096438, mae: 27.384136, mean_q: -40.248619\n",
      " 147121/150000: episode: 736, duration: 1.459s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.865 [0.000, 2.000],  loss: 2.441895, mae: 27.331135, mean_q: -40.149151\n",
      " 147321/150000: episode: 737, duration: 1.497s, episode steps: 200, steps per second: 134, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.985 [0.000, 2.000],  loss: 1.420304, mae: 27.407387, mean_q: -40.321777\n",
      " 147521/150000: episode: 738, duration: 1.461s, episode steps: 200, steps per second: 137, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.000 [0.000, 2.000],  loss: 2.456334, mae: 27.307226, mean_q: -40.107433\n",
      " 147721/150000: episode: 739, duration: 1.417s, episode steps: 200, steps per second: 141, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.950 [0.000, 2.000],  loss: 1.399666, mae: 27.214443, mean_q: -40.069206\n",
      " 147921/150000: episode: 740, duration: 1.425s, episode steps: 200, steps per second: 140, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 1.488714, mae: 27.301035, mean_q: -40.133698\n",
      " 148121/150000: episode: 741, duration: 1.484s, episode steps: 200, steps per second: 135, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 2.430739, mae: 27.231091, mean_q: -39.973728\n",
      " 148321/150000: episode: 742, duration: 1.446s, episode steps: 200, steps per second: 138, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.920 [0.000, 2.000],  loss: 1.892696, mae: 27.111092, mean_q: -39.828880\n",
      " 148521/150000: episode: 743, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.910 [0.000, 2.000],  loss: 1.306254, mae: 27.003870, mean_q: -39.680206\n",
      " 148683/150000: episode: 744, duration: 1.153s, episode steps: 162, steps per second: 140, episode reward: -162.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.049 [0.000, 2.000],  loss: 1.527585, mae: 27.179335, mean_q: -39.921135\n",
      " 148883/150000: episode: 745, duration: 1.411s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.045 [0.000, 2.000],  loss: 1.155327, mae: 27.055321, mean_q: -39.788506\n",
      " 149083/150000: episode: 746, duration: 1.407s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 1.065 [0.000, 2.000],  loss: 2.315809, mae: 27.079840, mean_q: -39.682785\n",
      " 149283/150000: episode: 747, duration: 1.411s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.935 [0.000, 2.000],  loss: 2.564691, mae: 27.102757, mean_q: -39.703144\n",
      " 149483/150000: episode: 748, duration: 1.413s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.980 [0.000, 2.000],  loss: 0.967972, mae: 26.882286, mean_q: -39.502472\n",
      " 149683/150000: episode: 749, duration: 1.408s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.845 [0.000, 2.000],  loss: 2.350887, mae: 26.716158, mean_q: -39.170525\n",
      " 149883/150000: episode: 750, duration: 1.410s, episode steps: 200, steps per second: 142, episode reward: -200.000, mean reward: -1.000 [-1.000, -1.000], mean action: 0.970 [0.000, 2.000],  loss: 1.264015, mae: 26.936224, mean_q: -39.589127\n",
      "done, took 1381.839 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6d9851cbe0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=150000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f12bfc9",
   "metadata": {},
   "source": [
    "# **DEL Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd4aca60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: -109.000, steps: 109\n",
      "Episode 2: reward: -112.000, steps: 112\n",
      "Episode 3: reward: -85.000, steps: 85\n",
      "Episode 4: reward: -111.000, steps: 111\n",
      "Episode 5: reward: -109.000, steps: 109\n",
      "Episode 6: reward: -112.000, steps: 112\n",
      "Episode 7: reward: -85.000, steps: 85\n",
      "Episode 8: reward: -200.000, steps: 200\n",
      "Episode 9: reward: -90.000, steps: 90\n",
      "Episode 10: reward: -108.000, steps: 108\n",
      "Episode 11: reward: -110.000, steps: 110\n",
      "Episode 12: reward: -110.000, steps: 110\n",
      "Episode 13: reward: -109.000, steps: 109\n",
      "Episode 14: reward: -108.000, steps: 108\n",
      "Episode 15: reward: -85.000, steps: 85\n",
      "Episode 16: reward: -87.000, steps: 87\n",
      "Episode 17: reward: -108.000, steps: 108\n",
      "Episode 18: reward: -112.000, steps: 112\n",
      "Episode 19: reward: -112.000, steps: 112\n",
      "Episode 20: reward: -92.000, steps: 92\n",
      "Episode 21: reward: -103.000, steps: 103\n",
      "Episode 22: reward: -111.000, steps: 111\n",
      "Episode 23: reward: -110.000, steps: 110\n",
      "Episode 24: reward: -84.000, steps: 84\n",
      "Episode 25: reward: -110.000, steps: 110\n",
      "Episode 26: reward: -108.000, steps: 108\n",
      "Episode 27: reward: -110.000, steps: 110\n",
      "Episode 28: reward: -109.000, steps: 109\n",
      "Episode 29: reward: -87.000, steps: 87\n",
      "Episode 30: reward: -93.000, steps: 93\n",
      "Episode 31: reward: -111.000, steps: 111\n",
      "Episode 32: reward: -85.000, steps: 85\n",
      "Episode 33: reward: -107.000, steps: 107\n",
      "Episode 34: reward: -110.000, steps: 110\n",
      "Episode 35: reward: -112.000, steps: 112\n",
      "Episode 36: reward: -85.000, steps: 85\n",
      "Episode 37: reward: -112.000, steps: 112\n",
      "Episode 38: reward: -93.000, steps: 93\n",
      "Episode 39: reward: -110.000, steps: 110\n",
      "Episode 40: reward: -110.000, steps: 110\n",
      "Episode 41: reward: -110.000, steps: 110\n",
      "Episode 42: reward: -112.000, steps: 112\n",
      "Episode 43: reward: -88.000, steps: 88\n",
      "Episode 44: reward: -84.000, steps: 84\n",
      "Episode 45: reward: -112.000, steps: 112\n",
      "Episode 46: reward: -110.000, steps: 110\n",
      "Episode 47: reward: -109.000, steps: 109\n",
      "Episode 48: reward: -108.000, steps: 108\n",
      "Episode 49: reward: -111.000, steps: 111\n",
      "Episode 50: reward: -197.000, steps: 197\n",
      "Episode 51: reward: -98.000, steps: 98\n",
      "Episode 52: reward: -112.000, steps: 112\n",
      "Episode 53: reward: -110.000, steps: 110\n",
      "Episode 54: reward: -88.000, steps: 88\n",
      "Episode 55: reward: -111.000, steps: 111\n",
      "Episode 56: reward: -95.000, steps: 95\n",
      "Episode 57: reward: -112.000, steps: 112\n",
      "Episode 58: reward: -110.000, steps: 110\n",
      "Episode 59: reward: -91.000, steps: 91\n",
      "Episode 60: reward: -92.000, steps: 92\n",
      "Episode 61: reward: -84.000, steps: 84\n",
      "Episode 62: reward: -88.000, steps: 88\n",
      "Episode 63: reward: -110.000, steps: 110\n",
      "Episode 64: reward: -109.000, steps: 109\n",
      "Episode 65: reward: -110.000, steps: 110\n",
      "Episode 66: reward: -112.000, steps: 112\n",
      "Episode 67: reward: -123.000, steps: 123\n",
      "Episode 68: reward: -112.000, steps: 112\n",
      "Episode 69: reward: -111.000, steps: 111\n",
      "Episode 70: reward: -92.000, steps: 92\n",
      "Episode 71: reward: -92.000, steps: 92\n",
      "Episode 72: reward: -85.000, steps: 85\n",
      "Episode 73: reward: -86.000, steps: 86\n",
      "Episode 74: reward: -84.000, steps: 84\n",
      "Episode 75: reward: -110.000, steps: 110\n",
      "Episode 76: reward: -98.000, steps: 98\n",
      "Episode 77: reward: -90.000, steps: 90\n",
      "Episode 78: reward: -96.000, steps: 96\n",
      "Episode 79: reward: -111.000, steps: 111\n",
      "Episode 80: reward: -112.000, steps: 112\n",
      "Episode 81: reward: -89.000, steps: 89\n",
      "Episode 82: reward: -110.000, steps: 110\n",
      "Episode 83: reward: -110.000, steps: 110\n",
      "Episode 84: reward: -109.000, steps: 109\n",
      "Episode 85: reward: -84.000, steps: 84\n",
      "Episode 86: reward: -109.000, steps: 109\n",
      "Episode 87: reward: -84.000, steps: 84\n",
      "Episode 88: reward: -85.000, steps: 85\n",
      "Episode 89: reward: -108.000, steps: 108\n",
      "Episode 90: reward: -86.000, steps: 86\n",
      "Episode 91: reward: -110.000, steps: 110\n",
      "Episode 92: reward: -111.000, steps: 111\n",
      "Episode 93: reward: -90.000, steps: 90\n",
      "Episode 94: reward: -85.000, steps: 85\n",
      "Episode 95: reward: -85.000, steps: 85\n",
      "Episode 96: reward: -103.000, steps: 103\n",
      "Episode 97: reward: -111.000, steps: 111\n",
      "Episode 98: reward: -111.000, steps: 111\n",
      "Episode 99: reward: -85.000, steps: 85\n",
      "Episode 100: reward: -95.000, steps: 95\n",
      "-103.49\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1993f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 15 episodes ...\n",
      "Episode 1: reward: -111.000, steps: 111\n",
      "Episode 2: reward: -110.000, steps: 110\n",
      "Episode 3: reward: -112.000, steps: 112\n",
      "Episode 4: reward: -91.000, steps: 91\n",
      "Episode 5: reward: -104.000, steps: 104\n",
      "Episode 6: reward: -110.000, steps: 110\n",
      "Episode 7: reward: -110.000, steps: 110\n",
      "Episode 8: reward: -109.000, steps: 109\n",
      "Episode 9: reward: -184.000, steps: 184\n",
      "Episode 10: reward: -110.000, steps: 110\n",
      "Episode 11: reward: -104.000, steps: 104\n",
      "Episode 12: reward: -108.000, steps: 108\n",
      "Episode 13: reward: -89.000, steps: 89\n",
      "Episode 14: reward: -91.000, steps: 91\n",
      "Episode 15: reward: -110.000, steps: 110\n"
     ]
    }
   ],
   "source": [
    "_ = dqn.test(env, nb_episodes=15, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8f25a",
   "metadata": {},
   "source": [
    "# **Reloading Agent from Memory (Saving Weights)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6794cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('dqn_weights_1.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df112370",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del dqn\n",
    "del env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6763b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "actions = env.action_space.n\n",
    "states = env.observation_space.shape[0]\n",
    "model = build_model(states, actions)\n",
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90c0e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('dqn_weights_1.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63ea0764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 15 episodes ...\n",
      "Episode 1: reward: -110.000, steps: 110\n",
      "Episode 2: reward: -85.000, steps: 85\n",
      "Episode 3: reward: -92.000, steps: 92\n",
      "Episode 4: reward: -108.000, steps: 108\n",
      "Episode 5: reward: -112.000, steps: 112\n",
      "Episode 6: reward: -103.000, steps: 103\n",
      "Episode 7: reward: -115.000, steps: 115\n",
      "Episode 8: reward: -85.000, steps: 85\n",
      "Episode 9: reward: -110.000, steps: 110\n",
      "Episode 10: reward: -88.000, steps: 88\n",
      "Episode 11: reward: -111.000, steps: 111\n",
      "Episode 12: reward: -109.000, steps: 109\n",
      "Episode 13: reward: -110.000, steps: 110\n",
      "Episode 14: reward: -112.000, steps: 112\n",
      "Episode 15: reward: -110.000, steps: 110\n"
     ]
    }
   ],
   "source": [
    "_ = dqn.test(env, nb_episodes=15, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
